{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        data = [item for item in data]\n",
    "    return data\n",
    "\n",
    "def create_df(data):\n",
    "    rows = []\n",
    "    for conv in data:\n",
    "        conv_id = conv[\"conversation_id\"]\n",
    "        history = conv[\"conversation_history\"]\n",
    "        \n",
    "        for model_name, response_data in conv[\"tutor_responses\"].items():\n",
    "            row = {\n",
    "                \"conversation_id\": conv_id,\n",
    "                \"conversation_history\": history,\n",
    "                \"model\": model_name,\n",
    "                \"response\": response_data[\"response\"],\n",
    "                \"mistake_identification\": response_data[\"annotation\"][\"Mistake_Identification\"],\n",
    "                \"mistake_location\": response_data[\"annotation\"][\"Mistake_Location\"],\n",
    "                \"providing_guidance\": response_data[\"annotation\"][\"Providing_Guidance\"],\n",
    "                \"actionability\": response_data[\"annotation\"][\"Actionability\"]\n",
    "            }\n",
    "            rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(df, column_name):\n",
    "    mapping = {\"No\": 0, \"To some extent\": 1, \"Yes\": 2}\n",
    "    return df[column_name].map(mapping)\n",
    "\n",
    "def tfidf_vectorize(X_train, X_val):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_val_tfidf = vectorizer.transform(X_val)\n",
    "    return X_train_tfidf, X_val_tfidf, vectorizer\n",
    "\n",
    "def get_class_weights(y_train):\n",
    "    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "    return class_weight_dict\n",
    "\n",
    "def train_and_eval(X, y, k, class_weight=None):\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    fold_reports = []\n",
    "    for _, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        X_train_tfidf, X_val_tfidf, vectorizer = tfidf_vectorize(X_train, X_val)\n",
    "        if class_weight is not None:\n",
    "            class_weight_dict = get_class_weights(y)\n",
    "        model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "        model.fit(X_train_tfidf, y_train)\n",
    "        y_pred = model.predict(X_val_tfidf)\n",
    "        report = classification_report(y_val, y_pred, output_dict=True)\n",
    "        fold_reports.append(report)\n",
    "    return model, vectorizer, fold_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('../data/mrbench_v3_devset.json')\n",
    "df = create_df(data)\n",
    "metrics = ['mistake_identification', 'mistake_location', 'providing_guidance', 'actionability']\n",
    "\n",
    "df[\"last_student_utterance\"] = df[\"conversation_history\"].apply(lambda x: x.split(\"\\n\")[-1].replace(\"Student: \", \"\"))\n",
    "df[\"input_text_1\"] = df[\"response\"] # macro accuracy 0.84 macro f1 0.64 with vectorizer ngram_range=(1, 2) (before i had 0.80, 0.60) - on 5 folds\n",
    "df[\"input_text_2\"] = df[\"last_student_utterance\"] + \" [SEP] \" + df[\"response\"] # macro accuracy 0.79 macro f1 0.62 (test train 0.8/0.2)\n",
    "df[\"input_text_3\"] = df[\"conversation_history\"] + \" [SEP] \" + df[\"response\"] # macro accuracy 0.55 macro f1 0.41 (test train 0.8/0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_report(fold_reports):\n",
    "    avg_report = {}\n",
    "    for fold_report in fold_reports:  # Iterate through folds\n",
    "        for key in fold_report.keys(): # Iterate through keys (0, 1, 2, accuracy, macro avg, weighted avg)\n",
    "            if isinstance(fold_report[key], dict):  # Handle per-class metrics\n",
    "                avg_report[key] = {}\n",
    "                for subkey in fold_report[key]: # Iterate through subkeys (precision, recall, f1-score, support)\n",
    "                    avg_report[key][subkey] = np.mean([report[key][subkey] for report in fold_reports])\n",
    "            else: # accuracy \n",
    "                avg_report[key] = np.mean([report[key] for report in fold_reports])\n",
    "\n",
    "    return avg_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF + TF-IDF + class weights\n",
    "def get_results(input_data, class_weight=True):\n",
    "    results = {}\n",
    "    for metric in metrics:\n",
    "        print(f\"\\nTraining model for: {metric}\")\n",
    "        df[\"labels\"] = encode_labels(df, metric)\n",
    "        model, vectorizer, fold_reports = train_and_eval(input_data, df[\"labels\"], k=5, class_weight=class_weight)\n",
    "        avg_report = get_avg_report(fold_reports)\n",
    "        results[metric] = {\n",
    "            \"model\": model,\n",
    "            \"vectorizer\": vectorizer\n",
    "        }\n",
    "        print(f\"Metric: {metric}\")\n",
    "        print(avg_report)\n",
    "        print(f'Macro accuracy: {avg_report[\"accuracy\"]} Macro F1: {avg_report[\"macro avg\"][\"f1-score\"]}')\n",
    "        print(\"----------------------------------------------------\")\n",
    "    return results\n",
    "\n",
    "import joblib\n",
    "def save_trained_models_and_vectorizers_for_inference(results, folder):\n",
    "    for metric, data in results.items():\n",
    "        model = data[\"model\"]\n",
    "        vectorizer = data[\"vectorizer\"]\n",
    "        joblib.dump(model, f\"../models/{folder}/{metric}_model.pkl\")\n",
    "        joblib.dump(vectorizer, f\"../models/{folder}/{metric}_vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for: mistake_identification\n",
      "Metric: mistake_identification\n",
      "{'0': {'precision': np.float64(0.9403551514445028), 'recall': np.float64(0.49729729729729727), 'f1-score': np.float64(0.6489694136887877), 'support': np.float64(74.0)}, '1': {'precision': np.float64(0.5732600732600732), 'recall': np.float64(0.12067226890756304), 'f1-score': np.float64(0.19638792102206737), 'support': np.float64(34.8)}, '2': {'precision': np.float64(0.852469672117374), 'recall': np.float64(0.9891272040808129), 'f1-score': np.float64(0.9156930616233272), 'support': np.float64(386.4)}, 'accuracy': np.float64(0.8546081785597914), 'macro avg': {'precision': np.float64(0.7886949656073166), 'recall': np.float64(0.5356989234285577), 'f1-score': np.float64(0.5870167987780607), 'support': np.float64(495.2)}, 'weighted avg': {'precision': np.float64(0.8459427876764802), 'recall': np.float64(0.8546081785597914), 'f1-score': np.float64(0.8252884340480519), 'support': np.float64(495.2)}}\n",
      "Macro accuracy: 0.8546081785597914 Macro F1: 0.5870167987780607\n",
      "----------------------------------------------------\n",
      "\n",
      "Training model for: mistake_location\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ana\\miniconda3\\envs\\deep-l\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ana\\miniconda3\\envs\\deep-l\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ana\\miniconda3\\envs\\deep-l\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ana\\miniconda3\\envs\\deep-l\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ana\\miniconda3\\envs\\deep-l\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ana\\miniconda3\\envs\\deep-l\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: mistake_location\n",
      "{'0': {'precision': np.float64(0.8668770198875908), 'recall': np.float64(0.40390032502708556), 'f1-score': np.float64(0.5493905479125067), 'support': np.float64(142.6)}, '1': {'precision': np.float64(0.13333333333333333), 'recall': np.float64(0.00909090909090909), 'f1-score': np.float64(0.01702127659574468), 'support': np.float64(44.0)}, '2': {'precision': np.float64(0.7037940335591595), 'recall': np.float64(0.9753772117849788), 'f1-score': np.float64(0.8175536500969128), 'support': np.float64(308.6)}, 'accuracy': np.float64(0.7249519387422614), 'macro avg': {'precision': np.float64(0.5680014622600279), 'recall': np.float64(0.4627894819676578), 'f1-score': np.float64(0.46132182486838813), 'support': np.float64(495.2)}, 'weighted avg': {'precision': np.float64(0.7000676349888894), 'recall': np.float64(0.7249519387422614), 'f1-score': np.float64(0.6692000091879396), 'support': np.float64(495.2)}}\n",
      "Macro accuracy: 0.7249519387422614 Macro F1: 0.46132182486838813\n",
      "----------------------------------------------------\n",
      "\n",
      "Training model for: providing_guidance\n",
      "Metric: providing_guidance\n",
      "{'0': {'precision': np.float64(0.8114078529955491), 'recall': np.float64(0.44698028256481914), 'f1-score': np.float64(0.5755807704606445), 'support': np.float64(113.2)}, '1': {'precision': np.float64(0.4267973856209151), 'recall': np.float64(0.049722772277227725), 'f1-score': np.float64(0.08853579556813891), 'support': np.float64(100.6)}, '2': {'precision': np.float64(0.6422871145633764), 'recall': np.float64(0.9609247621210975), 'f1-score': np.float64(0.7699049010935093), 'support': np.float64(281.4)}, 'accuracy': np.float64(0.6583186705767352), 'macro avg': {'precision': np.float64(0.6268307843932802), 'recall': np.float64(0.48587593898771464), 'f1-score': np.float64(0.4780071557074309), 'support': np.float64(495.2)}, 'weighted avg': {'precision': np.float64(0.6371543659992375), 'recall': np.float64(0.6583186705767352), 'f1-score': np.float64(0.587057220673584), 'support': np.float64(495.2)}}\n",
      "Macro accuracy: 0.6583186705767352 Macro F1: 0.4780071557074309\n",
      "----------------------------------------------------\n",
      "\n",
      "Training model for: actionability\n",
      "Metric: actionability\n",
      "{'0': {'precision': np.float64(0.7976587631348605), 'recall': np.float64(0.5495990566037736), 'f1-score': np.float64(0.6505033145732074), 'support': np.float64(159.4)}, '1': {'precision': np.float64(0.321994671994672), 'recall': np.float64(0.06227323213624584), 'f1-score': np.float64(0.10340699619230631), 'support': np.float64(73.8)}, '2': {'precision': np.float64(0.6500577269189256), 'recall': np.float64(0.9213740458015266), 'f1-score': np.float64(0.7622340458963326), 'support': np.float64(262.0)}, 'accuracy': np.float64(0.6736681329423264), 'macro avg': {'precision': np.float64(0.5899037206828194), 'recall': np.float64(0.5110821115138486), 'f1-score': np.float64(0.5053814522206155), 'support': np.float64(495.2)}, 'weighted avg': {'precision': np.float64(0.6487075133256853), 'recall': np.float64(0.6736681329423264), 'f1-score': np.float64(0.6280905297611352), 'support': np.float64(495.2)}}\n",
      "Macro accuracy: 0.6736681329423264 Macro F1: 0.5053814522206155\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results1 = get_results(df[\"input_text_1\"])\n",
    "# results2 = get_results(df[\"input_text_2\"])\n",
    "# results3 = get_results(df[\"input_text_3\"])\n",
    "# save_trained_models_and_vectorizers_for_inference(results1, 'RF_tfidf_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for: mistake_identification\n",
      "Metric: mistake_identification\n",
      "{'0': {'precision': np.float64(0.8980382051810623), 'recall': np.float64(0.5648648648648649), 'f1-score': np.float64(0.6921269089773013), 'support': np.float64(74.0)}, '1': {'precision': np.float64(0.5282051282051283), 'recall': np.float64(0.12100840336134455), 'f1-score': np.float64(0.1938685780092473), 'support': np.float64(34.8)}, '2': {'precision': np.float64(0.8642219709761733), 'recall': np.float64(0.9844693470431511), 'f1-score': np.float64(0.920403397248052), 'support': np.float64(386.4)}, 'accuracy': np.float64(0.8610703812316716), 'macro avg': {'precision': np.float64(0.7634884347874545), 'recall': np.float64(0.5567808717564536), 'f1-score': np.float64(0.6021329614115336), 'support': np.float64(495.2)}, 'weighted avg': {'precision': np.float64(0.8456355288633102), 'recall': np.float64(0.8610703812316716), 'f1-score': np.float64(0.835207123786067), 'support': np.float64(495.2)}}\n",
      "Macro accuracy: 0.8610703812316716 Macro F1: 0.6021329614115336\n",
      "----------------------------------------------------\n",
      "\n",
      "Training model for: mistake_location\n",
      "Metric: mistake_location\n",
      "{'0': {'precision': np.float64(0.8200129794959045), 'recall': np.float64(0.43474835024130803), 'f1-score': np.float64(0.5674188506398895), 'support': np.float64(142.6)}, '1': {'precision': np.float64(0.27999999999999997), 'recall': np.float64(0.022727272727272728), 'f1-score': np.float64(0.041688849452824604), 'support': np.float64(44.0)}, '2': {'precision': np.float64(0.7125535433721988), 'recall': np.float64(0.9604778716429202), 'f1-score': np.float64(0.8181088608361335), 'support': np.float64(308.6)}, 'accuracy': np.float64(0.7257624633431086), 'macro avg': {'precision': np.float64(0.6041888409560345), 'recall': np.float64(0.4726511648705003), 'f1-score': np.float64(0.47573885364294927), 'support': np.float64(495.2)}, 'weighted avg': {'precision': np.float64(0.705054305825418), 'recall': np.float64(0.7257624633431086), 'f1-score': np.float64(0.6769307718351237), 'support': np.float64(495.2)}}\n",
      "Macro accuracy: 0.7257624633431086 Macro F1: 0.47573885364294927\n",
      "----------------------------------------------------\n",
      "\n",
      "Training model for: providing_guidance\n",
      "Metric: providing_guidance\n",
      "{'0': {'precision': np.float64(0.7622943722943722), 'recall': np.float64(0.44875019406924394), 'f1-score': np.float64(0.5645158623546862), 'support': np.float64(113.2)}, '1': {'precision': np.float64(0.390551413482448), 'recall': np.float64(0.11342574257425742), 'f1-score': np.float64(0.17524912952736513), 'support': np.float64(100.6)}, '2': {'precision': np.float64(0.6510893297055845), 'recall': np.float64(0.9232452487317332), 'f1-score': np.float64(0.7636203209871592), 'support': np.float64(281.4)}, 'accuracy': np.float64(0.6502346041055718), 'macro avg': {'precision': np.float64(0.6013117051608016), 'recall': np.float64(0.49514039512507824), 'f1-score': np.float64(0.5011284376230701), 'support': np.float64(495.2)}, 'weighted avg': {'precision': np.float64(0.623521686527733), 'recall': np.float64(0.6502346041055718), 'f1-score': np.float64(0.5985440491555869), 'support': np.float64(495.2)}}\n",
      "Macro accuracy: 0.6502346041055718 Macro F1: 0.5011284376230701\n",
      "----------------------------------------------------\n",
      "\n",
      "Training model for: actionability\n",
      "Metric: actionability\n",
      "{'0': {'precision': np.float64(0.7856761465418239), 'recall': np.float64(0.5658569182389936), 'f1-score': np.float64(0.6574082245135655), 'support': np.float64(159.4)}, '1': {'precision': np.float64(0.3201135579396449), 'recall': np.float64(0.11640133283968904), 'f1-score': np.float64(0.16682921124972488), 'support': np.float64(73.8)}, '2': {'precision': np.float64(0.6595215705816497), 'recall': np.float64(0.8916030534351146), 'f1-score': np.float64(0.7581525692158332), 'support': np.float64(262.0)}, 'accuracy': np.float64(0.6712447051156728), 'macro avg': {'precision': np.float64(0.5884370916877062), 'recall': np.float64(0.5246204348379324), 'f1-score': np.float64(0.5274633349930412), 'support': np.float64(495.2)}, 'weighted avg': {'precision': np.float64(0.6495744063987106), 'recall': np.float64(0.6712447051156728), 'f1-score': np.float64(0.6376274050533824), 'support': np.float64(495.2)}}\n",
      "Macro accuracy: 0.6712447051156728 Macro F1: 0.5274633349930412\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# RF + TF-IDF + SMOTE \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "def get_results_smote(input_data):\n",
    "    results = {}\n",
    "    for metric in metrics:\n",
    "        print(f\"\\nTraining model for: {metric}\")\n",
    "        df[\"labels\"] = encode_labels(df, metric)\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        fold_reports = []\n",
    "        X = input_data\n",
    "        y = df[\"labels\"]\n",
    "\n",
    "        for train_idx, val_idx in skf.split(X, y):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "            X_train_tfidf, X_val_tfidf, vectorizer = tfidf_vectorize(X_train, X_val)\n",
    "            X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)\n",
    "            model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "            model.fit(X_train_resampled, y_train_resampled)\n",
    "            y_pred = model.predict(X_val_tfidf)\n",
    "            report = classification_report(y_val, y_pred, output_dict=True)\n",
    "            fold_reports.append(report)\n",
    "\n",
    "        avg_report = get_avg_report(fold_reports)\n",
    "        results[metric] = {\n",
    "            \"model\": model,\n",
    "            \"vectorizer\": vectorizer\n",
    "        }\n",
    "        print(f\"Metric: {metric}\")\n",
    "        print(avg_report)\n",
    "        print(f'Macro accuracy: {avg_report[\"accuracy\"]} Macro F1: {avg_report[\"macro avg\"][\"f1-score\"]}')\n",
    "        print(\"----------------------------------------------------\")\n",
    "    return results\n",
    "\n",
    "results_smote1 = get_results_smote(df[\"input_text_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ana\\miniconda3\\envs\\deep-l\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for: mistake_identification\n",
      "Metric: mistake_identification\n",
      "{'0': {'precision': np.float64(0.7950011622509429), 'recall': np.float64(0.6891891891891891), 'f1-score': np.float64(0.7376041492697952), 'support': np.float64(74.0)}, '1': {'precision': np.float64(0.3248323013415893), 'recall': np.float64(0.18436974789915964), 'f1-score': np.float64(0.2329899144749675), 'support': np.float64(34.8)}, '2': {'precision': np.float64(0.8907320709428277), 'recall': np.float64(0.9482427601719083), 'f1-score': np.float64(0.9185443311092689), 'support': np.float64(386.4)}, 'accuracy': np.float64(0.855818670576735), 'macro avg': {'precision': np.float64(0.6701885115117866), 'recall': np.float64(0.6072672324200857), 'f1-score': np.float64(0.6297127982846773), 'support': np.float64(495.2)}, 'weighted avg': {'precision': np.float64(0.8366574083669278), 'recall': np.float64(0.855818670576735), 'f1-score': np.float64(0.8433057075434434), 'support': np.float64(495.2)}}\n",
      "Macro accuracy: 0.855818670576735 Macro F1: 0.6297127982846773\n",
      "----------------------------------------------------\n",
      "\n",
      "Training model for: mistake_location\n",
      "Metric: mistake_location\n",
      "{'0': {'precision': np.float64(0.6754824935368376), 'recall': np.float64(0.6324534620309268), 'f1-score': np.float64(0.653053404552395), 'support': np.float64(142.6)}, '1': {'precision': np.float64(0.29175113274247816), 'recall': np.float64(0.23636363636363633), 'f1-score': np.float64(0.2592484320327458), 'support': np.float64(44.0)}, '2': {'precision': np.float64(0.7845452144125173), 'recall': np.float64(0.8275942504097843), 'f1-score': np.float64(0.8053822606929385), 'support': np.float64(308.6)}, 'accuracy': np.float64(0.7188888888888889), 'macro avg': {'precision': np.float64(0.5839262802306111), 'recall': np.float64(0.5654704496014492), 'f1-score': np.float64(0.5725613657593598), 'support': np.float64(495.2)}, 'weighted avg': {'precision': np.float64(0.7093571872138446), 'recall': np.float64(0.7188888888888889), 'f1-score': np.float64(0.7130039136620973), 'support': np.float64(495.2)}}\n",
      "Macro accuracy: 0.7188888888888889 Macro F1: 0.5725613657593598\n",
      "----------------------------------------------------\n",
      "\n",
      "Training model for: providing_guidance\n",
      "Metric: providing_guidance\n",
      "{'0': {'precision': np.float64(0.6460618114632591), 'recall': np.float64(0.5335351653469959), 'f1-score': np.float64(0.5836211139914516), 'support': np.float64(113.2)}, '1': {'precision': np.float64(0.39913334652137933), 'recall': np.float64(0.4333861386138613), 'f1-score': np.float64(0.414963679923527), 'support': np.float64(100.6)}, '2': {'precision': np.float64(0.7255264197778224), 'recall': np.float64(0.7526488478332197), 'f1-score': np.float64(0.7384112085911432), 'support': np.float64(281.4)}, 'accuracy': np.float64(0.6377134245682633), 'macro avg': {'precision': np.float64(0.5902405259208202), 'recall': np.float64(0.5731900505980256), 'f1-score': np.float64(0.5789986675020407), 'support': np.float64(495.2)}, 'weighted avg': {'precision': np.float64(0.6410723194002802), 'recall': np.float64(0.6377134245682633), 'f1-score': np.float64(0.6373298682129045), 'support': np.float64(495.2)}}\n",
      "Macro accuracy: 0.6377134245682633 Macro F1: 0.5789986675020407\n",
      "----------------------------------------------------\n",
      "\n",
      "Training model for: actionability\n",
      "Metric: actionability\n",
      "{'0': {'precision': np.float64(0.7383239817730437), 'recall': np.float64(0.6612264150943397), 'f1-score': np.float64(0.6974218629234649), 'support': np.float64(159.4)}, '1': {'precision': np.float64(0.3008368315329266), 'recall': np.float64(0.306442058496853), 'f1-score': np.float64(0.3018665930528778), 'support': np.float64(73.8)}, '2': {'precision': np.float64(0.7168321460702162), 'recall': np.float64(0.7587786259541984), 'f1-score': np.float64(0.7369570927537132), 'support': np.float64(262.0)}, 'accuracy': np.float64(0.659942163571196), 'macro avg': {'precision': np.float64(0.5853309864587288), 'recall': np.float64(0.5754823665151304), 'f1-score': np.float64(0.5787485162433519), 'support': np.float64(495.2)}, 'weighted avg': {'precision': np.float64(0.6617313607459201), 'recall': np.float64(0.659942163571196), 'f1-score': np.float64(0.6593669104052251), 'support': np.float64(495.2)}}\n",
      "Macro accuracy: 0.659942163571196 Macro F1: 0.5787485162433519\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# RF + TF-IDF + sentence piece tokenization\n",
    "import sentencepiece as spm\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "sp_model_path = hf_hub_download(repo_id=\"google/t5-v1_1-base\", filename=\"spiece.model\")\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(sp_model_path)\n",
    "\n",
    "df[\"tokenized_response\"] = df[\"response\"].apply(lambda x: \" \".join(sp.EncodeAsPieces(x)))\n",
    "\n",
    "results_sp1 = get_results(df[\"tokenized_response\"], class_weight=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ▁Great , ▁you ' ve ▁correctly ▁identified ▁the...\n",
       "1    ▁Now ▁that ▁we ▁know ▁the ▁cost ▁of ▁1 ▁ pound...\n",
       "2    ▁You ' re ▁close , ▁but ▁I ▁notice ▁that ▁you ...\n",
       "3    ▁That ' s ▁correct . ▁So , ▁ if ▁1 ▁ pound ▁of...\n",
       "4    ▁It ▁seems ▁like ▁you ' ve ▁calculated ▁the ▁c...\n",
       "Name: tokenized_response, dtype: object"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tokenized_response\"].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
