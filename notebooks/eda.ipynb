{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ana\\miniconda3\\envs\\deep-l\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 300/300 [00:38<00:00,  7.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>response_id</th>\n",
       "      <th>tutor</th>\n",
       "      <th>conversation_history</th>\n",
       "      <th>num_turns</th>\n",
       "      <th>last_student_turn</th>\n",
       "      <th>response</th>\n",
       "      <th>sentence_similarity</th>\n",
       "      <th>response_length</th>\n",
       "      <th>contains_question</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_questions</th>\n",
       "      <th>contains_question_words</th>\n",
       "      <th>mistake_identification</th>\n",
       "      <th>mistake_location</th>\n",
       "      <th>providing_guidance</th>\n",
       "      <th>actionability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e-Sonnet</td>\n",
       "      <td>Sonnet</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>6</td>\n",
       "      <td>The cost of 1 pound of meat is $7.00.</td>\n",
       "      <td>Great, you've correctly identified the cost of...</td>\n",
       "      <td>0.505259</td>\n",
       "      <td>141</td>\n",
       "      <td>False</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e-Llama...</td>\n",
       "      <td>Llama318B</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>6</td>\n",
       "      <td>The cost of 1 pound of meat is $7.00.</td>\n",
       "      <td>Now that we know the cost of 1 pound of meat i...</td>\n",
       "      <td>0.718742</td>\n",
       "      <td>175</td>\n",
       "      <td>False</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e-Llama...</td>\n",
       "      <td>Llama31405B</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>6</td>\n",
       "      <td>The cost of 1 pound of meat is $7.00.</td>\n",
       "      <td>You're close, but I notice that you calculated...</td>\n",
       "      <td>0.620214</td>\n",
       "      <td>225</td>\n",
       "      <td>True</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e-GPT4</td>\n",
       "      <td>GPT4</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>6</td>\n",
       "      <td>The cost of 1 pound of meat is $7.00.</td>\n",
       "      <td>That's correct. So, if 1 pound of meat costs $...</td>\n",
       "      <td>0.690453</td>\n",
       "      <td>169</td>\n",
       "      <td>True</td>\n",
       "      <td>44</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e-Mistral</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>6</td>\n",
       "      <td>The cost of 1 pound of meat is $7.00.</td>\n",
       "      <td>It seems like you've calculated the cost as if...</td>\n",
       "      <td>0.707090</td>\n",
       "      <td>148</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            conversation_id  \\\n",
       "0  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e   \n",
       "1  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e   \n",
       "2  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e   \n",
       "3  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e   \n",
       "4  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e   \n",
       "\n",
       "                                         response_id        tutor  \\\n",
       "0    221-362eb11a-f190-42a6-b2a4-985fafdcfa9e-Sonnet       Sonnet   \n",
       "1  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e-Llama...    Llama318B   \n",
       "2  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e-Llama...  Llama31405B   \n",
       "3      221-362eb11a-f190-42a6-b2a4-985fafdcfa9e-GPT4         GPT4   \n",
       "4   221-362eb11a-f190-42a6-b2a4-985fafdcfa9e-Mistral      Mistral   \n",
       "\n",
       "                                conversation_history  num_turns  \\\n",
       "0  Tutor: Hi, could you please provide a step-by-...          6   \n",
       "1  Tutor: Hi, could you please provide a step-by-...          6   \n",
       "2  Tutor: Hi, could you please provide a step-by-...          6   \n",
       "3  Tutor: Hi, could you please provide a step-by-...          6   \n",
       "4  Tutor: Hi, could you please provide a step-by-...          6   \n",
       "\n",
       "                       last_student_turn  \\\n",
       "0  The cost of 1 pound of meat is $7.00.   \n",
       "1  The cost of 1 pound of meat is $7.00.   \n",
       "2  The cost of 1 pound of meat is $7.00.   \n",
       "3  The cost of 1 pound of meat is $7.00.   \n",
       "4  The cost of 1 pound of meat is $7.00.   \n",
       "\n",
       "                                            response  sentence_similarity  \\\n",
       "0  Great, you've correctly identified the cost of...             0.505259   \n",
       "1  Now that we know the cost of 1 pound of meat i...             0.718742   \n",
       "2  You're close, but I notice that you calculated...             0.620214   \n",
       "3  That's correct. So, if 1 pound of meat costs $...             0.690453   \n",
       "4  It seems like you've calculated the cost as if...             0.707090   \n",
       "\n",
       "   response_length  contains_question  num_tokens  num_sentences  \\\n",
       "0              141              False          29              1   \n",
       "1              175              False          37              1   \n",
       "2              225               True          48              1   \n",
       "3              169               True          44              3   \n",
       "4              148              False          32              1   \n",
       "\n",
       "   num_questions  contains_question_words  mistake_identification  \\\n",
       "0              0                    False                       2   \n",
       "1              0                    False                       2   \n",
       "2              1                     True                       2   \n",
       "3              1                    False                       2   \n",
       "4              0                    False                       2   \n",
       "\n",
       "   mistake_location  providing_guidance  actionability  \n",
       "0                 2                   2              2  \n",
       "1                 1                   1              1  \n",
       "2                 2                   2              2  \n",
       "3                 2                   2              2  \n",
       "4                 2                   2              2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "class TutorDataLoader:\n",
    "    def __init__(self, data_path: str, model_name='all-MiniLM-L6-v2', nlp='en_core_web_sm') -> None:\n",
    "        self.data_path = data_path\n",
    "        self.raw_data = None\n",
    "        self.data_df = None\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.nlp = spacy.load(nlp)\n",
    "\n",
    "    def load_data(self) -> None:\n",
    "        with open(self.data_path) as f:\n",
    "            self.raw_data = json.load(f)\n",
    "\n",
    "    def process_data(self) -> pd.DataFrame:\n",
    "        data = []\n",
    "\n",
    "        for conv in tqdm(self.raw_data):\n",
    "            conv_id = conv['conversation_id']\n",
    "            history = conv['conversation_history']\n",
    "            conversation_turns = self._parse_conversation_history(history)\n",
    "            last_student_turn = next((turn['text'] for turn in reversed(conversation_turns) if turn['speaker'] == 'Student'), None)\n",
    "            \n",
    "            for tutor_name, tutor_data in conv['tutor_responses'].items():\n",
    "                response_text = tutor_data['response']\n",
    "                annotation = tutor_data.get('annotation', {})\n",
    "                features = self._get_linguisitc_features(response_text)\n",
    "\n",
    "                row = {\n",
    "                    'conversation_id': conv_id,\n",
    "                    'response_id': f\"{conv_id}-{tutor_name}\",\n",
    "                    'tutor': tutor_name,\n",
    "                    'conversation_history': history,\n",
    "                    'num_turns': len(conversation_turns),\n",
    "                    'last_student_turn': last_student_turn,\n",
    "                    'response': response_text,\n",
    "                    'sentence_similarity': self._calculate_sentence_similarity(last_student_turn, response_text),\n",
    "                    'response_length': len(response_text),\n",
    "                    'contains_question': '?' in response_text,\n",
    "                    'num_tokens': features['num_tokens'],\n",
    "                    'num_sentences': features['num_sentences'],\n",
    "                    'num_questions': features['num_questions'],\n",
    "                    'contains_question_words': features['contains_question_words'],\n",
    "                }\n",
    "\n",
    "                for dimension, value in annotation.items():\n",
    "                    row[f\"{dimension.lower()}\"] = 0 if value == 'No' else (1 if value == 'To some extent' else 2)\n",
    "\n",
    "                data.append(row)\n",
    "\n",
    "        self.data_df = pd.DataFrame(data)\n",
    "        return self.data_df\n",
    "\n",
    "    def _parse_conversation_history(self, history: str) -> list:\n",
    "        turns = []\n",
    "        parts = re.split(r'(Tutor:|Student:)', history)\n",
    "        current_speaker = None\n",
    "\n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "            if part == 'Tutor:':\n",
    "                current_speaker = 'Tutor'\n",
    "            elif part == 'Student:':\n",
    "                current_speaker = 'Student'\n",
    "            elif part and current_speaker:\n",
    "                turns.append({'speaker': current_speaker, 'text': part})\n",
    "\n",
    "        return turns\n",
    "\n",
    "    def _calculate_sentence_similarity(self, sentence1: str, sentence2: str) -> float:\n",
    "        \"\"\"Compute cosine similarity between two sentences using SBERT embeddings.\"\"\"\n",
    "        if not sentence1 or not sentence2:\n",
    "            return 0.0 \n",
    "\n",
    "        embeddings = self.model.encode([sentence1, sentence2], convert_to_tensor=True)\n",
    "        similarity = util.pytorch_cos_sim(embeddings[0], embeddings[1])\n",
    "        \n",
    "        return similarity.item()\n",
    "\n",
    "    def _get_linguisitc_features(self, text: str) -> dict:\n",
    "        doc = self.nlp(text)\n",
    "    \n",
    "        features = {\n",
    "            'num_tokens': len(doc),\n",
    "            'num_sentences': len(list(doc.sents)),\n",
    "            'num_questions': sum(1 for token in doc if token.text in ['?']),\n",
    "            'contains_question_words': any(token.text.lower() in ['what', 'why', 'how', 'when', 'where'] for token in doc),\n",
    "        }\n",
    "\n",
    "        return features\n",
    "\n",
    "loader = TutorDataLoader(\"../data/mrbench_v3_devset.json\")\n",
    "loader.load_data()\n",
    "final_df = loader.process_data()\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"../data/df_eda.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive EDA...\n",
      "Analyzing cross-tutor aggregate features...\n",
      "Analysis complete. Results saved to eda_results/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "class TutorAnalysis:\n",
    "    def __init__(self, data: pd.DataFrame, output_dir='eda_results') -> None:\n",
    "        self.data = data\n",
    "        self.output_dir = output_dir\n",
    "        self.dimensions = [\"Mistake_Identification\", \"Mistake_Location\", \"Providing_Guidance\", \"Actionability\"]\n",
    "        sns.set(style=\"whitegrid\")\n",
    "\n",
    "    def run_complete_analysis(self) -> None:\n",
    "        print(\"Starting comprehensive EDA...\")\n",
    "        # self._basic_dataset_overview()\n",
    "        # self._annotation_distributions()\n",
    "        # self._annotation_correlations()\n",
    "        # self._plot_tutor_radar()\n",
    "        # self._linguistic_features_by_tutor()\n",
    "        # self._feature_breakdown_by_annotation_and_tutor()\n",
    "        # self._ngram_content_analysis()\n",
    "        # self._plot_ngram_bars_by_annotation()\n",
    "        # self._log_odds_ratio_ngrams()\n",
    "        # self._top_ngrams_by_tutor()\n",
    "        # self._identify_boilerplate_ngrams()\n",
    "        # self._feature_importance_by_annotation()\n",
    "        # self._per_model_feature_annotation_correlations()\n",
    "        self._cross_tutor_feature_correlations()\n",
    "\n",
    "        print(f\"Analysis complete. Results saved to {self.output_dir}/\")\n",
    "\n",
    "    def _basic_dataset_overview(self) -> None:\n",
    "        print(\"Basic dataset overview...\")\n",
    "        _, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Number of responses per tutor\n",
    "        tutor_counts = self.data['tutor'].value_counts()\n",
    "        tutor_counts.plot(kind='bar', ax=axes[0, 0], color='steelblue')\n",
    "        axes[0, 0].set_title('Number of Responses per Tutor')\n",
    "        axes[0, 0].set_ylabel('Count')\n",
    "        \n",
    "        # Distribution of conversation turn counts\n",
    "        self.data['num_turns'].plot(kind='hist', bins=20, ax=axes[0, 1], color='steelblue')\n",
    "        axes[0, 1].set_title('Distribution of Conversation Turn Counts')\n",
    "        axes[0, 1].set_xlabel('Number of Turns')\n",
    "        \n",
    "        # Response length distribution\n",
    "        self.data['response_length'].plot(kind='hist', bins=50, ax=axes[1, 0], color='steelblue')\n",
    "        axes[1, 0].set_title('Distribution of Response Lengths')\n",
    "        axes[1, 0].set_xlabel('Characters')\n",
    "        \n",
    "        # Response length by tutor\n",
    "        sns.boxplot(x='tutor', y='response_length', data=self.data, ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Response Length by Tutor')\n",
    "        axes[1, 1].set_ylabel('Characters')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.output_dir}/dataset_overview.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def _annotation_distributions(self) -> None:\n",
    "        print(\"Annotation distributions...\")\n",
    "        plt.figure(figsize=(14, 10))\n",
    "  \n",
    "        for i, dim in enumerate(self.dimensions, 1):\n",
    "            plt.subplot(2, 2, i)\n",
    "            dim_data = self.data[[f\"{dim.lower()}\"]].copy()\n",
    "            counts = dim_data.value_counts().sort_index(ascending=False)\n",
    "            \n",
    "            ax = counts.plot(kind='bar', color=['#2ecc71', '#f39c12', '#e74c3c'])\n",
    "            ax.set_title(f'Distribution of {dim}')\n",
    "            ax.set_ylabel('Count')\n",
    "            \n",
    "            # Add percentage labels\n",
    "            total = counts.sum()\n",
    "            for _, p in enumerate(ax.patches):\n",
    "                percentage = f'{100 * p.get_height() / total:.1f}%'\n",
    "                ax.annotate(percentage, (p.get_x() + p.get_width() / 2, p.get_height()),\n",
    "                           ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.output_dir}/annotation_distributions.png\", dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def _annotation_correlations(self) -> None:\n",
    "        print(\"Analyzing annotation correlations...\")\n",
    "        annotation_columns = [f\"{dim.lower()}\" for dim in self.dimensions]\n",
    "        corr = self.data[annotation_columns].corr()\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "        cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "        \n",
    "        sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
    "                   annot=True, fmt='.2f', square=True, linewidths=.5)\n",
    "        \n",
    "        plt.title('Correlation between Annotation Dimensions')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.output_dir}/annotation_correlations.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def _plot_tutor_radar(self) -> None:\n",
    "        print(\"Generating radar plots for tutor effectiveness...\")\n",
    "        outdir = os.path.join(self.output_dir, \"tutor_radar_plot\")\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "        tutor_scores = self.data.groupby('tutor')[[\n",
    "        'mistake_identification', 'mistake_location', 'providing_guidance', 'actionability']].mean()\n",
    "        # Normalize scores to [0, 1]\n",
    "        tutor_scores_norm = tutor_scores / 2.0\n",
    "\n",
    "        labels = tutor_scores_norm.columns.tolist()\n",
    "        num_vars = len(labels)\n",
    "\n",
    "        angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "        angles += angles[:1]  # Close the loop\n",
    "\n",
    "        _, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "        for tutor_name, values in tutor_scores_norm.iterrows():\n",
    "            values = values.tolist()\n",
    "            values += values[:1]  # Close the loop\n",
    "            \n",
    "            ax.plot(angles, values, linewidth=2, label=tutor_name)\n",
    "            ax.fill(angles, values, alpha=0.25)\n",
    "\n",
    "        ax.set_yticks([0, 0.5, 1])\n",
    "        ax.set_yticklabels(['0', '1', '2'], fontsize=9)\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(labels, fontsize=10)\n",
    "        ax.set_title(\"Tutor Effectiveness Radar\", fontsize=14)\n",
    "        ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.1))\n",
    "        ax.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(outdir, \"combined_radar_plot.png\"), dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def _linguistic_features_by_tutor(self) -> None:\n",
    "        print(\"Analyzing linguistic features by tutor...\")\n",
    "\n",
    "        plt.figure(figsize=(12, 10))\n",
    "\n",
    "        plt.subplot(2, 2, 1)\n",
    "        sns.boxplot(x='tutor', y='num_tokens', data=self.data)\n",
    "        plt.title('Number of Tokens by Tutor')\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        sns.boxplot(x='tutor', y='num_sentences', data=self.data)\n",
    "        plt.title('Number of Sentences by Tutor')\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        sns.boxplot(x='tutor', y='num_questions', data=self.data)\n",
    "        plt.title('Number of Questions by Tutor')\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        plt.subplot(2, 2, 4)\n",
    "        sns.countplot(x='tutor', hue='contains_question_words', data=self.data)\n",
    "        plt.title('Usage of Question Words by Tutor')\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('linguistic_features_by_tutor.png')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.output_dir}/linguistic_features_by_tutor.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def _feature_breakdown_by_annotation_and_tutor(self) -> None:\n",
    "        print(\"Analyzing features across annotation levels and tutors...\")\n",
    "\n",
    "        feature_vars = ['response_length', 'num_questions', 'sentence_similarity']\n",
    "        binary_var = 'contains_question'\n",
    "        \n",
    "        for dim in self.dimensions:\n",
    "            label = dim.lower()\n",
    "            print(f\" → Processing: {dim}\")\n",
    "            \n",
    "            fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "            fig.suptitle(f'{dim}: Feature Distribution by Label and Tutor', fontsize=16)\n",
    "\n",
    "            # Plot 1: Response Length\n",
    "            sns.boxplot(x=label, y=feature_vars[0], hue='tutor', data=self.data, ax=axes[0, 0])\n",
    "            axes[0, 0].set_title('Response Length')\n",
    "            axes[0, 0].set_xlabel(dim)\n",
    "            axes[0, 0].set_ylabel(\"Characters\")\n",
    "\n",
    "            # Plot 2: Number of Questions\n",
    "            sns.boxplot(x=label, y=feature_vars[1], hue='tutor', data=self.data, ax=axes[0, 1])\n",
    "            axes[0, 1].set_title('Number of Questions')\n",
    "            axes[0, 1].set_xlabel(dim)\n",
    "            axes[0, 1].set_ylabel(\"Count\")\n",
    "\n",
    "            # Plot 3: Sentence Similarity\n",
    "            sns.boxplot(x=label, y=feature_vars[2], hue='tutor', data=self.data, ax=axes[1, 0])\n",
    "            axes[1, 0].set_title('Sentence Similarity with Last Student Turn')\n",
    "            axes[1, 0].set_xlabel(dim)\n",
    "            axes[1, 0].set_ylabel(\"Cosine Similarity\")\n",
    "\n",
    "            # Plot 4: Contains Question (Bar %)\n",
    "            bin_df = self.data.groupby([label, 'tutor'])[binary_var].mean().reset_index()\n",
    "            sns.barplot(x=label, y=binary_var, hue='tutor', data=bin_df, ax=axes[1, 1])\n",
    "            axes[1, 1].set_title('Contains Question (%)')\n",
    "            axes[1, 1].set_xlabel(dim)\n",
    "            axes[1, 1].set_ylabel(\"Proportion\")\n",
    "\n",
    "            for ax in axes.flat:\n",
    "                ax.set_xticks([0, 1, 2])\n",
    "                ax.set_xticklabels(['No', 'To some extent', 'Yes'])\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "            plt.savefig(f\"{self.output_dir}/feature_breakdown_{label}_by_tutor.png\", dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "    def _plot_ngram_bars_by_annotation(self, ngram_range=(2, 3), top_k=15) -> None:\n",
    "        print(\"Generating bar charts for top n-grams...\")\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        from collections import Counter\n",
    "        import os\n",
    "\n",
    "        outdir = os.path.join(self.output_dir, \"ngram_barplots\")\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "        for dim in self.dimensions:\n",
    "            dim_col = dim.lower()\n",
    "            high_df = self.data[self.data[dim_col] == 2]\n",
    "            low_df = self.data[self.data[dim_col] == 0]\n",
    "\n",
    "            # Texts\n",
    "            high_texts = high_df['response'].dropna().tolist()\n",
    "            low_texts = low_df['response'].dropna().tolist()\n",
    "\n",
    "            # High\n",
    "            vect = CountVectorizer(ngram_range=ngram_range, stop_words='english', lowercase=True)\n",
    "            high_ngrams = vect.fit_transform(high_texts)\n",
    "            high_counts = Counter(dict(zip(vect.get_feature_names_out(), high_ngrams.sum(axis=0).A1)))\n",
    "            high_top = high_counts.most_common(top_k)\n",
    "\n",
    "            # Low\n",
    "            vect = CountVectorizer(ngram_range=ngram_range, stop_words='english', lowercase=True)\n",
    "            low_ngrams = vect.fit_transform(low_texts)\n",
    "            low_counts = Counter(dict(zip(vect.get_feature_names_out(), low_ngrams.sum(axis=0).A1)))\n",
    "            low_top = low_counts.most_common(top_k)\n",
    "\n",
    "            # Plot both\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "            fig.suptitle(f\"Top N-grams for {dim}\", fontsize=16)\n",
    "\n",
    "            high_labels, high_vals = zip(*high_top)\n",
    "            axes[0].barh(high_labels[::-1], high_vals[::-1], color='green')\n",
    "            axes[0].set_title(\"High Score (2)\")\n",
    "            axes[0].set_xlabel(\"Frequency\")\n",
    "\n",
    "            low_labels, low_vals = zip(*low_top)\n",
    "            axes[1].barh(low_labels[::-1], low_vals[::-1], color='red')\n",
    "            axes[1].set_title(\"Low Score (0)\")\n",
    "            axes[1].set_xlabel(\"Frequency\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(outdir, f\"{dim_col}_ngram_barplot.png\"), dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "    def _log_odds_ratio_ngrams(self, ngram_range=(1, 3), top_k=20) -> None:\n",
    "        print(\"Computing log-odds ratio for discriminative n-grams...\")\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        import numpy as np\n",
    "        import os\n",
    "\n",
    "        outdir = os.path.join(self.output_dir, \"log_odds_ngrams\")\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "        def compute_log_odds(p1, p2):\n",
    "            eps = 1e-5\n",
    "            return np.log((p1 + eps) / (1 - p1 + eps)) - np.log((p2 + eps) / (1 - p2 + eps))\n",
    "\n",
    "        for dim in self.dimensions:\n",
    "            dim_col = dim.lower()\n",
    "            high_df = self.data[self.data[dim_col] == 2]\n",
    "            low_df = self.data[self.data[dim_col] == 0]\n",
    "\n",
    "            vect = CountVectorizer(ngram_range=ngram_range, stop_words='english', lowercase=True)\n",
    "            X_all = vect.fit_transform(self.data['response'])\n",
    "            X_high = vect.transform(high_df['response'])\n",
    "            X_low = vect.transform(low_df['response'])\n",
    "\n",
    "            total = X_all.sum(axis=0).A1\n",
    "            p_total = total / total.sum()\n",
    "\n",
    "            p_high = X_high.sum(axis=0).A1 / X_high.sum()\n",
    "            p_low = X_low.sum(axis=0).A1 / X_low.sum()\n",
    "\n",
    "            log_odds_scores = compute_log_odds(p_high, p_low)\n",
    "            terms = vect.get_feature_names_out()\n",
    "            sorted_idx = np.argsort(log_odds_scores)[::-1]\n",
    "\n",
    "            with open(os.path.join(outdir, f\"{dim_col}_log_odds_top.txt\"), \"w\") as f:\n",
    "                f.write(f\"Top {top_k} phrases for HIGH vs LOW in {dim}\\n\")\n",
    "                for i in sorted_idx[:top_k]:\n",
    "                    f.write(f\"{terms[i]}: {log_odds_scores[i]:.4f}\\n\")\n",
    "\n",
    "    def _top_ngrams_by_tutor(self, ngram_range=(2, 3), top_k=10) -> None:\n",
    "        print(\"Extracting top n-grams per tutor model...\")\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        from collections import Counter\n",
    "        import os\n",
    "\n",
    "        outdir = os.path.join(self.output_dir, \"ngrams_by_tutor\")\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "        for tutor in self.data['tutor'].unique():\n",
    "            responses = self.data[self.data['tutor'] == tutor]['response'].dropna().tolist()\n",
    "            vect = CountVectorizer(ngram_range=ngram_range, stop_words='english')\n",
    "            X = vect.fit_transform(responses)\n",
    "            ngram_freq = Counter(dict(zip(vect.get_feature_names_out(), X.sum(axis=0).A1)))\n",
    "            top_ngrams = ngram_freq.most_common(top_k)\n",
    "\n",
    "            with open(os.path.join(outdir, f\"{tutor}_ngrams.txt\"), \"w\") as f:\n",
    "                for phrase, count in top_ngrams:\n",
    "                    f.write(f\"{phrase}: {count}\\n\")\n",
    "    \n",
    "    def _identify_boilerplate_ngrams(self, ngram_range=(2, 3), top_k=30) -> None:\n",
    "        \"\"\"\n",
    "        Identifies common n-grams across all tutors and all annotation scores,\n",
    "        which are likely to be boilerplate (uninformative) phrases.\n",
    "        \"\"\"\n",
    "        print(\"Identifying common boilerplate phrases across the dataset...\")\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        from collections import Counter\n",
    "        import os\n",
    "\n",
    "        outdir = os.path.join(self.output_dir, \"boilerplate\")\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "        # Collect n-grams across all responses\n",
    "        responses = self.data['response'].dropna().tolist()\n",
    "        vectorizer = CountVectorizer(ngram_range=ngram_range, stop_words='english', lowercase=True)\n",
    "        X = vectorizer.fit_transform(responses)\n",
    "        total_counts = Counter(dict(zip(vectorizer.get_feature_names_out(), X.sum(axis=0).A1)))\n",
    "        most_common_all = total_counts.most_common(top_k)\n",
    "\n",
    "        # Save \n",
    "        boilerplate_file = os.path.join(outdir, f\"common_ngrams_top{top_k}.txt\")\n",
    "        with open(boilerplate_file, \"w\") as f:\n",
    "            f.write(\"Most Common N-grams (likely boilerplate)\\n\")\n",
    "            for phrase, count in most_common_all:\n",
    "                f.write(f\"{phrase}: {count}\\n\")\n",
    "\n",
    "    def _feature_importance_by_annotation(self) -> None:\n",
    "        \"\"\"\n",
    "        Computes feature importance using mutual information between features and annotation labels.\n",
    "        Saves bar plots showing the most informative features per label.\n",
    "        \"\"\"\n",
    "        print(\"Running feature importance analysis using Mutual Information...\")\n",
    "        from sklearn.feature_selection import mutual_info_classif\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        import os\n",
    "\n",
    "        outdir = os.path.join(self.output_dir, \"feature_importance\")\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "        feature_cols = ['response_length', 'num_sentences', 'num_questions',\n",
    "                        'sentence_similarity', 'contains_question', \n",
    "                        'num_tokens', 'contains_question_words']\n",
    "        \n",
    "        feature_data = self.data[feature_cols]\n",
    "\n",
    "        for dim in self.dimensions:\n",
    "            label_col = dim.lower()\n",
    "            y = self.data[label_col]\n",
    "\n",
    "            # Compute mutual information\n",
    "            mi_scores = mutual_info_classif(feature_data, y, discrete_features='auto', random_state=42)\n",
    "            feature_importance = pd.Series(mi_scores, index=feature_cols).sort_values(ascending=True)\n",
    "\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            feature_importance.plot(kind='barh', color='teal')\n",
    "            plt.title(f\"Mutual Information: Features vs. {dim}\")\n",
    "            plt.xlabel(\"Mutual Information Score\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(outdir, f\"feature_importance_{label_col}.png\"), dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "    def _per_model_feature_annotation_correlations(self) -> None:\n",
    "        \"\"\"\n",
    "        Computes correlation between linguistic features and annotation scores for each tutor.\n",
    "        Saves a heatmap per tutor to reveal model-specific feature-behavior patterns.\n",
    "        \"\"\"\n",
    "        print(\"Analyzing per-model feature-to-annotation correlations...\")\n",
    "        import os\n",
    "\n",
    "        outdir = os.path.join(self.output_dir, \"tutor_feature_correlations\")\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "        feature_cols = ['response_length', 'num_sentences', 'num_questions',\n",
    "                        'sentence_similarity', 'contains_question', \n",
    "                        'num_tokens', 'contains_question_words']\n",
    "        target_cols = [dim.lower() for dim in self.dimensions]\n",
    "\n",
    "        for tutor in self.data['tutor'].unique():\n",
    "            tutor_df = self.data[self.data['tutor'] == tutor]\n",
    "            corr_df = tutor_df[feature_cols + target_cols].corr().loc[feature_cols, target_cols]\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.heatmap(corr_df, annot=True, cmap='vlag', center=0, linewidths=0.5)\n",
    "            plt.title(f\"Feature ↔ Annotation Correlation: {tutor}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(outdir, f\"{tutor}_feature_annotation_corr.png\"), dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "    def _cross_tutor_feature_correlations(self) -> None:\n",
    "        \"\"\"\n",
    "        Computes correlation between linguistic features and annotation scores across all tutors.\n",
    "        Saves a heatmap to reveal global feature-behavior patterns.\n",
    "        \"\"\"\n",
    "        print(\"Analyzing cross-tutor aggregate features...\")\n",
    "        df = self.data.copy()\n",
    "\n",
    "        # Compute per-conversation maxima\n",
    "        df['max_response_length'] = df.groupby('conversation_id')['response_length'].transform('max')\n",
    "        df['max_sentence_similarity'] = df.groupby('conversation_id')['sentence_similarity'].transform('max')\n",
    "        df['max_num_questions'] = df.groupby('conversation_id')['num_questions'].transform('max')\n",
    "\n",
    "        # Binary features: is this the max value in group?\n",
    "        df['is_longest'] = (df['response_length'] == df['max_response_length']).astype(int)\n",
    "        df['is_most_similar'] = (df['sentence_similarity'] == df['max_sentence_similarity']).astype(int)\n",
    "        df['is_most_questioning'] = (df['num_questions'] == df['max_num_questions']).astype(int)\n",
    "\n",
    "        # Store to CSV for inspection if needed\n",
    "        df[['conversation_id', 'tutor', 'is_longest', 'is_most_similar', 'is_most_questioning']].to_csv(\n",
    "            f\"{self.output_dir}/cross_tutor_flags.csv\", index=False\n",
    "        )\n",
    "\n",
    "        # Evaluate how those binary indicators relate to pedagogical scores\n",
    "        features = ['is_longest', 'is_most_similar', 'is_most_questioning']\n",
    "        dim_labels = [dim.lower() for dim in self.dimensions]\n",
    "\n",
    "        plot_data = []\n",
    "\n",
    "        for feature in features:\n",
    "            for dim in dim_labels:\n",
    "                subset = df[df[feature] == 1]\n",
    "                mean = subset[dim].mean()\n",
    "                plot_data.append({\n",
    "                    'feature': feature,\n",
    "                    'dimension': dim,\n",
    "                    'mean_score': mean\n",
    "                })\n",
    "\n",
    "        plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "        # Pivot for heatmap\n",
    "        heatmap_data = plot_df.pivot(index='feature', columns='dimension', values='mean_score')\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.heatmap(heatmap_data, annot=True, cmap='YlGnBu', fmt=\".2f\")\n",
    "        plt.title(\"Average Pedagogical Score by Aggregated Cross-Tutor Feature\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.output_dir}/cross_tutor_feature_heatmap.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Save table\n",
    "        plot_df.to_csv(f\"{self.output_dir}/cross_tutor_feature_table.csv\", index=False)\n",
    "\n",
    "\n",
    "analysis = TutorAnalysis(final_df)\n",
    "analysis.run_complete_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram Barplots conclusions\n",
    "1) Certain response styles (e.g., “great job”, “let’s try”, “total number”) are ubiquitous across the dataset regardless of annotation quality.\n",
    "- They lack discriminative power between dimensions and labels, these n-grams are popular but not pedagogically informative.\n",
    "\n",
    "I may need to filter out boilerplate phrases that occur frequently in both low and high scores when building classifiers.\n",
    "\n",
    "2) Many n-grams in the low score bar also appear in the Phi3 tutor’s frequent n-grams.\n",
    "=> Often fails to identify and locate mistakes & provide guidance/actionable steps, leading to low scores.\n",
    "Use textual pattern frequency per tutor per label as feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phi feature correlation heatmap\n",
    "- High Sentence Similarity = High Scores\n",
    "\n",
    "Phi3’s highest positive correlation is with sentence similarity, especially for Mistake Identification (+0.34), it tends to echo or paraphrase the student’s utterance rather than correcting it.\n",
    "- Longer = Worse. Negative correlation between response length / sentence count / num tokens and all annotation scores.\n",
    "- Number of questions is weakly or negatively correlated with all annotation labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novice feature correlation heatmap\n",
    "- high correlations for response_length (0.64), num_sentences (0.58), num_tokens (0.63) with Providing_Guidance\n",
    "\n",
    "Expert\n",
    "- Strongest correlation with Actionability via contains_question (0.44) and num_questions (0.43), implying expert tutors engage learners via targeted questioning.\n",
    "\n",
    "General trends:\n",
    "- num_questions and contains_question are helpful especially for Actionability and Providing_Guidance, but only for some tutors (Expert, GPT4, Sonnet).\n",
    "\n",
    "Divergent behavior:\n",
    "- num_questions has positive correlations with annotations for most of them.\n",
    "but is negatively correlated for Phi3, Novice and Mistral — suggesting some models may use questions rhetorically or ineffectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd say we have great insights to incorporate tutor-awareness in your classification models, but the problem is that at inference time, I will be in a semi-supervised setting, I don’t know the tutor ID, but you receive multiple responses per conversation, one per (unknown) tutor.\n",
    "\n",
    "This means I cannot directly use tutor-specific features or heads at inference, unless\n",
    "1) I predict the tutor identity first, or\n",
    "2) I design a model to implicitly learn tutor-like clusters without knowing the tutor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
