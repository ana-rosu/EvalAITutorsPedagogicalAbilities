{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        data = [item for item in data]\n",
    "    return data\n",
    "\n",
    "def create_df(data):\n",
    "    rows = []\n",
    "    for conv in data:\n",
    "        conv_id = conv[\"conversation_id\"]\n",
    "        history = conv[\"conversation_history\"]\n",
    "        \n",
    "        for model_name, response_data in conv[\"tutor_responses\"].items():\n",
    "            row = {\n",
    "                \"conversation_id\": conv_id,\n",
    "                \"conversation_history\": history,\n",
    "                \"model\": model_name,\n",
    "                \"response\": response_data[\"response\"],\n",
    "                \"mistake_identification\": response_data[\"annotation\"][\"Mistake_Identification\"],\n",
    "                \"mistake_location\": response_data[\"annotation\"][\"Mistake_Location\"],\n",
    "                \"providing_guidance\": response_data[\"annotation\"][\"Providing_Guidance\"],\n",
    "                \"actionability\": response_data[\"annotation\"][\"Actionability\"]\n",
    "            }\n",
    "            rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(df, column_name):\n",
    "    mapping = {\"No\": 0, \"To some extent\": 1, \"Yes\": 2}\n",
    "    return df[column_name].map(mapping)\n",
    "\n",
    "def tfidf_vectorize(X_train, X_val):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_val_tfidf = vectorizer.transform(X_val)\n",
    "    return X_train_tfidf, X_val_tfidf, vectorizer\n",
    "\n",
    "def get_class_weights(y_train):\n",
    "    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "    return class_weight_dict\n",
    "\n",
    "def train_and_eval(X, y, k, class_weight=None):\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    fold_reports = []\n",
    "    for _, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        X_train_tfidf, X_val_tfidf, vectorizer = tfidf_vectorize(X_train, X_val)\n",
    "        if class_weight is not None:\n",
    "            class_weight_dict = get_class_weights(y)\n",
    "        model = SVC(kernel='linear', class_weight=class_weight_dict)\n",
    "        model.fit(X_train_tfidf, y_train)\n",
    "        y_pred = model.predict(X_val_tfidf)\n",
    "        report = classification_report(y_val, y_pred, output_dict=True)\n",
    "        fold_reports.append(report)\n",
    "    return model, vectorizer, fold_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('../data/mrbench_v3_devset.json')\n",
    "df = create_df(data)\n",
    "metrics = ['mistake_identification', 'mistake_location', 'providing_guidance', 'actionability']\n",
    "\n",
    "df[\"last_student_utterance\"] = df[\"conversation_history\"].apply(lambda x: x.split(\"\\n\")[-1].replace(\"Student: \", \"\"))\n",
    "df[\"input_text_1\"] = df[\"response\"] # macro accuracy 0.84 macro f1 0.64 with vectorizer ngram_range=(1, 2) (before i had 0.80, 0.60) - on 5 folds\n",
    "df[\"input_text_2\"] = df[\"last_student_utterance\"] + \" [SEP] \" + df[\"response\"] # macro accuracy 0.79 macro f1 0.62 (test train 0.8/0.2)\n",
    "df[\"input_text_3\"] = df[\"conversation_history\"] + \" [SEP] \" + df[\"response\"] # macro accuracy 0.55 macro f1 0.41 (test train 0.8/0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_report(fold_reports):\n",
    "    avg_report = {}\n",
    "    for fold_report in fold_reports:  # Iterate through folds\n",
    "        for key in fold_report.keys(): # Iterate through keys (0, 1, 2, accuracy, macro avg, weighted avg)\n",
    "            if isinstance(fold_report[key], dict):  # Handle per-class metrics\n",
    "                avg_report[key] = {}\n",
    "                for subkey in fold_report[key]: # Iterate through subkeys (precision, recall, f1-score, support)\n",
    "                    avg_report[key][subkey] = np.mean([report[key][subkey] for report in fold_reports])\n",
    "            else: # accuracy \n",
    "                avg_report[key] = np.mean([report[key] for report in fold_reports])\n",
    "\n",
    "    return avg_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC + TF-IDF + class weights\n",
    "def get_results(input_data, class_weight=True):\n",
    "    results = {}\n",
    "    for metric in metrics:\n",
    "        print(f\"\\nTraining model for: {metric}\")\n",
    "        df[\"labels\"] = encode_labels(df, metric)\n",
    "        model, vectorizer, fold_reports = train_and_eval(input_data, df[\"labels\"], k=5, class_weight=class_weight)\n",
    "        avg_report = get_avg_report(fold_reports)\n",
    "        results[metric] = {\n",
    "            \"model\": model,\n",
    "            \"vectorizer\": vectorizer\n",
    "        }\n",
    "        print(f\"Metric: {metric}\")\n",
    "        print(avg_report)\n",
    "        print(f'Macro accuracy: {avg_report[\"accuracy\"]} Macro F1: {avg_report[\"macro avg\"][\"f1-score\"]}')\n",
    "        print(\"----------------------------------------------------\")\n",
    "    return results\n",
    "\n",
    "import joblib\n",
    "def save_trained_models_and_vectorizers_for_inference(results, folder):\n",
    "    for metric, data in results.items():\n",
    "        model = data[\"model\"]\n",
    "        vectorizer = data[\"vectorizer\"]\n",
    "        joblib.dump(model, f\"../models/{folder}/{metric}_model.pkl\")\n",
    "        joblib.dump(vectorizer, f\"../models/{folder}/{metric}_vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for: mistake_identification\n",
      "Metric: mistake_identification\n",
      "{'0': {'precision': np.float64(0.7878098033842089), 'recall': np.float64(0.6972972972972973), 'f1-score': np.float64(0.7390455375065341), 'support': np.float64(74.0)}, '1': {'precision': np.float64(0.3378968253968254), 'recall': np.float64(0.19008403361344536), 'f1-score': np.float64(0.2406764288186364), 'support': np.float64(34.8)}, '2': {'precision': np.float64(0.8917925422990566), 'recall': np.float64(0.9461742378599831), 'f1-score': np.float64(0.9181145771368096), 'support': np.float64(386.4)}, 'accuracy': np.float64(0.8558170413815575), 'macro avg': {'precision': np.float64(0.6724997236933636), 'recall': np.float64(0.6111851895902419), 'f1-score': np.float64(0.6326121811539933), 'support': np.float64(495.2)}, 'weighted avg': {'precision': np.float64(0.8373369444820881), 'recall': np.float64(0.8558170413815575), 'f1-score': np.float64(0.843728768303594), 'support': np.float64(495.2)}}\n",
      "Macro accuracy: 0.8558170413815575 Macro F1: 0.6326121811539933\n",
      "----------------------------------------------------\n",
      "\n",
      "Training model for: mistake_location\n",
      "Metric: mistake_location\n",
      "{'0': {'precision': np.float64(0.6710855588802133), 'recall': np.float64(0.6394760169408057), 'f1-score': np.float64(0.6544887488818316), 'support': np.float64(142.6)}, '1': {'precision': np.float64(0.26918885136053144), 'recall': np.float64(0.21363636363636368), 'f1-score': np.float64(0.2359433854031577), 'support': np.float64(44.0)}, '2': {'precision': np.float64(0.7817931569274279), 'recall': np.float64(0.8211154541251628), 'f1-score': np.float64(0.8008385290946054), 'support': np.float64(308.6)}, 'accuracy': np.float64(0.7148525578364289), 'macro avg': {'precision': np.float64(0.5740225223893909), 'recall': np.float64(0.5580759449007774), 'f1-score': np.float64(0.5637568877931982), 'support': np.float64(495.2)}, 'weighted avg': {'precision': np.float64(0.7043715962663795), 'recall': np.float64(0.7148525578364289), 'f1-score': np.float64(0.7085143419829489), 'support': np.float64(495.2)}}\n",
      "Macro accuracy: 0.7148525578364289 Macro F1: 0.5637568877931982\n",
      "----------------------------------------------------\n",
      "\n",
      "Training model for: providing_guidance\n",
      "Metric: providing_guidance\n",
      "{'0': {'precision': np.float64(0.6553021754568727), 'recall': np.float64(0.5335506908865083), 'f1-score': np.float64(0.5873936199630089), 'support': np.float64(113.2)}, '1': {'precision': np.float64(0.3996419708387077), 'recall': np.float64(0.4333465346534653), 'f1-score': np.float64(0.4152213726549488), 'support': np.float64(100.6)}, '2': {'precision': np.float64(0.7256535514500528), 'recall': np.float64(0.7569268822089297), 'f1-score': np.float64(0.7405383288705919), 'support': np.float64(281.4)}, 'accuracy': np.float64(0.6401401107852721), 'macro avg': {'precision': np.float64(0.5935325659152111), 'recall': np.float64(0.574608035916301), 'f1-score': np.float64(0.5810511071628499), 'support': np.float64(495.2)}, 'weighted avg': {'precision': np.float64(0.6433645822852462), 'recall': np.float64(0.6401401107852721), 'f1-score': np.float64(0.6394557466268243), 'support': np.float64(495.2)}}\n",
      "Macro accuracy: 0.6401401107852721 Macro F1: 0.5810511071628499\n",
      "----------------------------------------------------\n",
      "\n",
      "Training model for: actionability\n",
      "Metric: actionability\n",
      "{'0': {'precision': np.float64(0.7287516894652275), 'recall': np.float64(0.6649921383647799), 'f1-score': np.float64(0.6951525629596531), 'support': np.float64(159.4)}, '1': {'precision': np.float64(0.30670365468797717), 'recall': np.float64(0.3090707145501666), 'f1-score': np.float64(0.3057443941522293), 'support': np.float64(73.8)}, '2': {'precision': np.float64(0.718990539751981), 'recall': np.float64(0.7557251908396947), 'f1-score': np.float64(0.736657197046575), 'support': np.float64(262.0)}, 'accuracy': np.float64(0.6599405343760183), 'macro avg': {'precision': np.float64(0.5848152946350619), 'recall': np.float64(0.5765960145848804), 'f1-score': np.float64(0.5791847180528192), 'support': np.float64(495.2)}, 'weighted avg': {'precision': np.float64(0.6606787976432799), 'recall': np.float64(0.6599405343760183), 'f1-score': np.float64(0.6590664037756648), 'support': np.float64(495.2)}}\n",
      "Macro accuracy: 0.6599405343760183 Macro F1: 0.5791847180528192\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results1 = get_results(df[\"input_text_1\"])\n",
    "# results2 = get_results(df[\"input_text_2\"])\n",
    "# results3 = get_results(df[\"input_text_3\"])\n",
    "# save_trained_models_and_vectorizers_for_inference(results1, 'SVC_tfidf_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for: mistake_identification\n",
      "Metric: mistake_identification\n",
      "{'0': {'precision': np.float64(0.8361011453120373), 'recall': np.float64(0.6702702702702703), 'f1-score': np.float64(0.7434916011667243), 'support': np.float64(74.0)}, '1': {'precision': np.float64(0.3478894634776987), 'recall': np.float64(0.1557983193277311), 'f1-score': np.float64(0.21040578742549187), 'support': np.float64(34.8)}, '2': {'precision': np.float64(0.8839440480964493), 'recall': np.float64(0.9611814007042347), 'f1-score': np.float64(0.9208919272548748), 'support': np.float64(386.4)}, 'accuracy': np.float64(0.8610671228413164), 'macro avg': {'precision': np.float64(0.689311552295395), 'recall': np.float64(0.595749996767412), 'f1-score': np.float64(0.6249297719490304), 'support': np.float64(495.2)}, 'weighted avg': {'precision': np.float64(0.839115798181884), 'recall': np.float64(0.8610671228413164), 'f1-score': np.float64(0.8444150816834475), 'support': np.float64(495.2)}}\n",
      "Macro accuracy: 0.8610671228413164 Macro F1: 0.6249297719490304\n",
      "----------------------------------------------------\n",
      "\n",
      "Training model for: mistake_location\n",
      "Metric: mistake_location\n",
      "{'0': {'precision': np.float64(0.6990319484437132), 'recall': np.float64(0.6016251354279524), 'f1-score': np.float64(0.6462562729322178), 'support': np.float64(142.6)}, '1': {'precision': np.float64(0.277308095952024), 'recall': np.float64(0.15), 'f1-score': np.float64(0.193476868190317), 'support': np.float64(44.0)}, '2': {'precision': np.float64(0.7677064405344445), 'recall': np.float64(0.8690833438406255), 'f1-score': np.float64(0.8152327620162222), 'support': np.float64(308.6)}, 'accuracy': np.float64(0.728178559791463), 'macro avg': {'precision': np.float64(0.5813488283100605), 'recall': np.float64(0.5402361597561927), 'f1-score': np.float64(0.5516553010462524), 'support': np.float64(495.2)}, 'weighted avg': {'precision': np.float64(0.7043693615665974), 'recall': np.float64(0.728178559791463), 'f1-score': np.float64(0.7113378443538073), 'support': np.float64(495.2)}}\n",
      "Macro accuracy: 0.728178559791463 Macro F1: 0.5516553010462524\n",
      "----------------------------------------------------\n",
      "\n",
      "Training model for: providing_guidance\n",
      "Metric: providing_guidance\n",
      "{'0': {'precision': np.float64(0.6418964803312629), 'recall': np.float64(0.5212078869740724), 'f1-score': np.float64(0.5747586271101712), 'support': np.float64(113.2)}, '1': {'precision': np.float64(0.41901170419035266), 'recall': np.float64(0.3458217821782178), 'f1-score': np.float64(0.378177099198097), 'support': np.float64(100.6)}, '2': {'precision': np.float64(0.711153774259231), 'recall': np.float64(0.808813507988188), 'f1-score': np.float64(0.7566734114716913), 'support': np.float64(281.4)}, 'accuracy': np.float64(0.649028999674161), 'macro avg': {'precision': np.float64(0.5906873195936155), 'recall': np.float64(0.5586143923801594), 'f1-score': np.float64(0.5698697125933199), 'support': np.float64(495.2)}, 'weighted avg': {'precision': np.float64(0.6359978077782971), 'recall': np.float64(0.649028999674161), 'f1-score': np.float64(0.6382160380037396), 'support': np.float64(495.2)}}\n",
      "Macro accuracy: 0.649028999674161 Macro F1: 0.5698697125933199\n",
      "----------------------------------------------------\n",
      "\n",
      "Training model for: actionability\n",
      "Metric: actionability\n",
      "{'0': {'precision': np.float64(0.7326081817945278), 'recall': np.float64(0.6574685534591195), 'f1-score': np.float64(0.6929116384113894), 'support': np.float64(159.4)}, '1': {'precision': np.float64(0.32969951857894797), 'recall': np.float64(0.25209181784524254), 'f1-score': np.float64(0.28237595587802194), 'support': np.float64(73.8)}, '2': {'precision': np.float64(0.7119419634790192), 'recall': np.float64(0.8030534351145038), 'f1-score': np.float64(0.7544311481994874), 'support': np.float64(262.0)}, 'accuracy': np.float64(0.6740729879439556), 'macro avg': {'precision': np.float64(0.5914165546174983), 'recall': np.float64(0.5708712688062886), 'f1-score': np.float64(0.5765729141629662), 'support': np.float64(495.2)}, 'weighted avg': {'precision': np.float64(0.6616087408494393), 'recall': np.float64(0.6740729879439556), 'f1-score': np.float64(0.6642654507822416), 'support': np.float64(495.2)}}\n",
      "Macro accuracy: 0.6740729879439556 Macro F1: 0.5765729141629662\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression + TF-IDF + SMOTE \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "def get_results_smote(input_data):\n",
    "    results = {}\n",
    "    for metric in metrics:\n",
    "        print(f\"\\nTraining model for: {metric}\")\n",
    "        df[\"labels\"] = encode_labels(df, metric)\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        fold_reports = []\n",
    "        X = input_data\n",
    "        y = df[\"labels\"]\n",
    "\n",
    "        for train_idx, val_idx in skf.split(X, y):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "            X_train_tfidf, X_val_tfidf, vectorizer = tfidf_vectorize(X_train, X_val)\n",
    "            X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)\n",
    "            model = SVC(kernel='linear', class_weight='balanced')\n",
    "            model.fit(X_train_resampled, y_train_resampled)\n",
    "            y_pred = model.predict(X_val_tfidf)\n",
    "            report = classification_report(y_val, y_pred, output_dict=True)\n",
    "            fold_reports.append(report)\n",
    "\n",
    "        avg_report = get_avg_report(fold_reports)\n",
    "        results[metric] = {\n",
    "            \"model\": model,\n",
    "            \"vectorizer\": vectorizer\n",
    "        }\n",
    "        print(f\"Metric: {metric}\")\n",
    "        print(avg_report)\n",
    "        print(f'Macro accuracy: {avg_report[\"accuracy\"]} Macro F1: {avg_report[\"macro avg\"][\"f1-score\"]}')\n",
    "        print(\"----------------------------------------------------\")\n",
    "    return results\n",
    "\n",
    "results_smote1 = get_results_smote(df[\"input_text_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ana\\miniconda3\\envs\\deep-l\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for: mistake_identification\n",
      "Metric: mistake_identification\n",
      "{'0': {'precision': np.float64(0.7950011622509429), 'recall': np.float64(0.6891891891891891), 'f1-score': np.float64(0.7376041492697952), 'support': np.float64(74.0)}, '1': {'precision': np.float64(0.3248323013415893), 'recall': np.float64(0.18436974789915964), 'f1-score': np.float64(0.2329899144749675), 'support': np.float64(34.8)}, '2': {'precision': np.float64(0.8907320709428277), 'recall': np.float64(0.9482427601719083), 'f1-score': np.float64(0.9185443311092689), 'support': np.float64(386.4)}, 'accuracy': np.float64(0.855818670576735), 'macro avg': {'precision': np.float64(0.6701885115117866), 'recall': np.float64(0.6072672324200857), 'f1-score': np.float64(0.6297127982846773), 'support': np.float64(495.2)}, 'weighted avg': {'precision': np.float64(0.8366574083669278), 'recall': np.float64(0.855818670576735), 'f1-score': np.float64(0.8433057075434434), 'support': np.float64(495.2)}}\n",
      "Macro accuracy: 0.855818670576735 Macro F1: 0.6297127982846773\n",
      "----------------------------------------------------\n",
      "\n",
      "Training model for: mistake_location\n",
      "Metric: mistake_location\n",
      "{'0': {'precision': np.float64(0.6754824935368376), 'recall': np.float64(0.6324534620309268), 'f1-score': np.float64(0.653053404552395), 'support': np.float64(142.6)}, '1': {'precision': np.float64(0.29175113274247816), 'recall': np.float64(0.23636363636363633), 'f1-score': np.float64(0.2592484320327458), 'support': np.float64(44.0)}, '2': {'precision': np.float64(0.7845452144125173), 'recall': np.float64(0.8275942504097843), 'f1-score': np.float64(0.8053822606929385), 'support': np.float64(308.6)}, 'accuracy': np.float64(0.7188888888888889), 'macro avg': {'precision': np.float64(0.5839262802306111), 'recall': np.float64(0.5654704496014492), 'f1-score': np.float64(0.5725613657593598), 'support': np.float64(495.2)}, 'weighted avg': {'precision': np.float64(0.7093571872138446), 'recall': np.float64(0.7188888888888889), 'f1-score': np.float64(0.7130039136620973), 'support': np.float64(495.2)}}\n",
      "Macro accuracy: 0.7188888888888889 Macro F1: 0.5725613657593598\n",
      "----------------------------------------------------\n",
      "\n",
      "Training model for: providing_guidance\n",
      "Metric: providing_guidance\n",
      "{'0': {'precision': np.float64(0.6460618114632591), 'recall': np.float64(0.5335351653469959), 'f1-score': np.float64(0.5836211139914516), 'support': np.float64(113.2)}, '1': {'precision': np.float64(0.39913334652137933), 'recall': np.float64(0.4333861386138613), 'f1-score': np.float64(0.414963679923527), 'support': np.float64(100.6)}, '2': {'precision': np.float64(0.7255264197778224), 'recall': np.float64(0.7526488478332197), 'f1-score': np.float64(0.7384112085911432), 'support': np.float64(281.4)}, 'accuracy': np.float64(0.6377134245682633), 'macro avg': {'precision': np.float64(0.5902405259208202), 'recall': np.float64(0.5731900505980256), 'f1-score': np.float64(0.5789986675020407), 'support': np.float64(495.2)}, 'weighted avg': {'precision': np.float64(0.6410723194002802), 'recall': np.float64(0.6377134245682633), 'f1-score': np.float64(0.6373298682129045), 'support': np.float64(495.2)}}\n",
      "Macro accuracy: 0.6377134245682633 Macro F1: 0.5789986675020407\n",
      "----------------------------------------------------\n",
      "\n",
      "Training model for: actionability\n",
      "Metric: actionability\n",
      "{'0': {'precision': np.float64(0.7383239817730437), 'recall': np.float64(0.6612264150943397), 'f1-score': np.float64(0.6974218629234649), 'support': np.float64(159.4)}, '1': {'precision': np.float64(0.3008368315329266), 'recall': np.float64(0.306442058496853), 'f1-score': np.float64(0.3018665930528778), 'support': np.float64(73.8)}, '2': {'precision': np.float64(0.7168321460702162), 'recall': np.float64(0.7587786259541984), 'f1-score': np.float64(0.7369570927537132), 'support': np.float64(262.0)}, 'accuracy': np.float64(0.659942163571196), 'macro avg': {'precision': np.float64(0.5853309864587288), 'recall': np.float64(0.5754823665151304), 'f1-score': np.float64(0.5787485162433519), 'support': np.float64(495.2)}, 'weighted avg': {'precision': np.float64(0.6617313607459201), 'recall': np.float64(0.659942163571196), 'f1-score': np.float64(0.6593669104052251), 'support': np.float64(495.2)}}\n",
      "Macro accuracy: 0.659942163571196 Macro F1: 0.5787485162433519\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SVC + TF-IDF + sentence piece tokenization\n",
    "import sentencepiece as spm\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "sp_model_path = hf_hub_download(repo_id=\"google/t5-v1_1-base\", filename=\"spiece.model\")\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(sp_model_path)\n",
    "\n",
    "df[\"tokenized_response\"] = df[\"response\"].apply(lambda x: \" \".join(sp.EncodeAsPieces(x)))\n",
    "\n",
    "results_sp1 = get_results(df[\"tokenized_response\"], class_weight=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ▁Great , ▁you ' ve ▁correctly ▁identified ▁the...\n",
       "1    ▁Now ▁that ▁we ▁know ▁the ▁cost ▁of ▁1 ▁ pound...\n",
       "2    ▁You ' re ▁close , ▁but ▁I ▁notice ▁that ▁you ...\n",
       "3    ▁That ' s ▁correct . ▁So , ▁ if ▁1 ▁ pound ▁of...\n",
       "4    ▁It ▁seems ▁like ▁you ' ve ▁calculated ▁the ▁c...\n",
       "Name: tokenized_response, dtype: object"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tokenized_response\"].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
