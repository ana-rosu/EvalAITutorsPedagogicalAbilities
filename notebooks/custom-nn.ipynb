{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\"../data/df_eda.csv\")\n",
    "\n",
    "# --- Basic Linguistic Features (already present in df) ---\n",
    "# ['response_length', 'num_sentences', 'num_tokens', 'num_questions',\n",
    "#  'contains_question', 'contains_question_words', 'sentence_similarity']\n",
    "\n",
    "# --- Normalize length features within each conversation_id (Z-score) ---\n",
    "length_features = ['response_length', 'sentence_similarity', 'num_questions']\n",
    "for feature in length_features:\n",
    "    df[f'{feature}_z'] = df.groupby('conversation_id')[feature].transform(lambda x: (x - x.mean()) / (x.std() + 1e-6))\n",
    "\n",
    "# --- Add interaction feature ---\n",
    "df['question_density'] = df['num_questions'] / (df['num_sentences'] + 1e-6)\n",
    "df['question_similarity_interaction'] = df['contains_question'].astype(int) * df['sentence_similarity']\n",
    "\n",
    "# --- Boilerplate / top n-grams as binary flags ---\n",
    "boilerplate_phrases = [\n",
    "    \"great job\", \"let try\", \"total number\", \"looks like\", \"right track\", \"closer look\",\n",
    "    \"let closer\", \"small mistake\", \"let closer look\", \"let think\", \"good try\", \"let look\",\n",
    "    \"double check\", \"tutor response\", \"job let\", \"great job let\", \"total cost\", \"think small\",\n",
    "    \"effort let\", \"let double\", \"let double check\", \"response maximum\", \"tutor response maximum\",\n",
    "    \"great start\", \"make sure\", \"maximum sentence\", \"response maximum sentence\", \"great try\",\n",
    "    \"assistant tutor\", \"assistant tutor response\"\n",
    "]\n",
    "\n",
    "# Preprocessing\n",
    "def contains_phrase(text, phrase):\n",
    "    return int(bool(re.search(rf'\\b{re.escape(phrase)}\\b', text.lower())))\n",
    "\n",
    "for phrase in boilerplate_phrases:\n",
    "    flag_name = f\"has_{phrase.replace(' ', '_')}\"\n",
    "    df[flag_name] = df['response'].apply(lambda x: contains_phrase(x, phrase))\n",
    "\n",
    "# --- Aggregated cross-tutor features ---\n",
    "agg_features = ['sentence_similarity', 'response_length', 'num_questions']\n",
    "aggregations = df.groupby('conversation_id')[agg_features].agg(['mean', 'std', 'min', 'max'])\n",
    "aggregations.columns = ['_'.join(col) for col in aggregations.columns]\n",
    "df = df.merge(aggregations, left_on='conversation_id', right_index=True, how='left')\n",
    "\n",
    "# --- One-hot encoding tutor for tutor-aware training ---\n",
    "df = pd.get_dummies(df, columns=['tutor'], prefix='tutor')\n",
    "\n",
    "# Save final feature set\n",
    "df.to_csv(\"../data/final_features.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tutor_columns = [col for col in df.columns if col.startswith('tutor_')]\n",
    "df['tutor_label'] = df[tutor_columns].idxmax(axis=1).str.replace('tutor_', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# TF-IDF vectorization\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), stop_words='english', max_features=300)\n",
    "tfidf_matrix = tfidf.fit_transform(df['response'])\n",
    "\n",
    "# Dimensionality reduction (optional but recommended)\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "tfidf_reduced = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Add back to df\n",
    "tfidf_df = pd.DataFrame(tfidf_reduced, columns=[f\"tfidf_{i}\" for i in range(tfidf_reduced.shape[1])])\n",
    "df = pd.concat([df.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TutorClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, dropout=0.3, num_tutors=9):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.head_mistake_identification = nn.Linear(hidden_dim // 2, 3)\n",
    "        self.head_mistake_location = nn.Linear(hidden_dim // 2, 3)\n",
    "        self.head_providing_guidance = nn.Linear(hidden_dim // 2, 3)\n",
    "        self.head_actionability = nn.Linear(hidden_dim // 2, 3)\n",
    "\n",
    "        # Optional tutor prediction head\n",
    "        self.head_tutor = nn.Linear(hidden_dim // 2, num_tutors)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared(x)\n",
    "        return {\n",
    "            'mistake_identification': self.head_mistake_identification(shared),\n",
    "            'mistake_location': self.head_mistake_location(shared),\n",
    "            'providing_guidance': self.head_providing_guidance(shared),\n",
    "            'actionability': self.head_actionability(shared),\n",
    "            'tutor': self.head_tutor(shared)  # optional\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class TutorDataset(Dataset):\n",
    "    def __init__(self, dataframe, feature_cols, label_cols, tutor_col=None):\n",
    "        self.features = dataframe[feature_cols].values.astype('float32')\n",
    "        self.labels = dataframe[label_cols].values.astype('int64')\n",
    "        self.tutors = None\n",
    "        if tutor_col:\n",
    "            self.tutors = dataframe[tutor_col].astype('category').cat.codes.values.astype('int64')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label_names = ['mistake_identification', 'mistake_location', 'providing_guidance', 'actionability']\n",
    "        \n",
    "        label_dict = {\n",
    "            name: torch.tensor(self.labels[idx][i], dtype=torch.long)\n",
    "            for i, name in enumerate(label_names)\n",
    "        }\n",
    "\n",
    "        sample = {\n",
    "            'features': torch.tensor(self.features[idx]),\n",
    "            'labels': label_dict\n",
    "        }\n",
    "\n",
    "        if self.tutors is not None:\n",
    "            sample['tutor'] = torch.tensor(self.tutors[idx], dtype=torch.long)\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BERT embeddings: (2476, 768)\n",
      "2476\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load embeddings\n",
    "bert_embeddings = np.load(\"../data/embeddings/X_bert_mean.npy\")  # shape: (num_rows, embedding_dim)\n",
    "\n",
    "bert_dim = bert_embeddings.shape[1]\n",
    "bert_df = pd.DataFrame(bert_embeddings, columns=[f'bert_{i}' for i in range(bert_dim)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "feature_cols = [\n",
    "    col for col in df.columns\n",
    "    if (col.startswith(\"tfidf_\") or col not in [\n",
    "        'conversation_id', 'response_id',\n",
    "        'mistake_identification', 'mistake_location',\n",
    "        'providing_guidance', 'actionability'\n",
    "    ])\n",
    "]\n",
    "feature_cols += [col for col in bert_df.columns]\n",
    "label_cols = ['mistake_identification', 'mistake_location', 'providing_guidance', 'actionability']\n",
    "tutor_col = 'tutor_label'\n",
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Final feature columns: numeric and not labels\n",
    "feature_cols = [col for col in numeric_cols if col not in label_cols]\n",
    "# Initialize dataset\n",
    "dataset = TutorDataset(df, feature_cols, label_cols, tutor_col=tutor_col)\n",
    "\n",
    "# Train/Val Split\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = {}\n",
    "for dim in label_cols:\n",
    "    weights = compute_class_weight('balanced', classes=np.unique(df[dim]), y=df[dim])\n",
    "    class_weights[dim] = torch.tensor(weights, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fns = {dim: nn.CrossEntropyLoss(weight=class_weights[dim]) for dim in class_weights}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, use_tutor_head=False):\n",
    "    model.eval()\n",
    "    dimension_keys = ['mistake_identification', 'mistake_location', 'providing_guidance', 'actionability']\n",
    "    y_true = {dim: [] for dim in dimension_keys}\n",
    "    y_pred = {dim: [] for dim in dimension_keys}\n",
    "\n",
    "    tutor_true = []\n",
    "    tutor_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x = batch['features']\n",
    "            y = batch['labels']\n",
    "            t = batch['tutor'] if use_tutor_head else None\n",
    "\n",
    "            outputs = model(x)\n",
    "            task_outputs = {k: outputs[k] for k in ['mistake_identification', 'mistake_location', 'providing_guidance', 'actionability']}\n",
    "            tutor_output = outputs['tutor'] if use_tutor_head else None\n",
    "            \n",
    "\n",
    "            for dim in dimension_keys:\n",
    "                preds = torch.argmax(task_outputs[dim], dim=1).cpu().numpy()\n",
    "                y_pred[dim].extend(preds)\n",
    "                y_true[dim].extend(y[dim].cpu().numpy())\n",
    "\n",
    "            if use_tutor_head and tutor_output is not None:\n",
    "                tutor_pred.extend(torch.argmax(tutor_output, dim=1).cpu().numpy())\n",
    "                tutor_true.extend(t.cpu().numpy())\n",
    "\n",
    "\n",
    "    for dim in dimension_keys:\n",
    "        print(f\"\\n Classification Report for {dim}\")\n",
    "        print(classification_report(y_true[dim], y_pred[dim], digits=3, zero_division=0))\n",
    "\n",
    "    if use_tutor_head:\n",
    "        print(\"\\n Tutor Classification Report\")\n",
    "        print(classification_report(tutor_true, tutor_pred, digits=3, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, use_tutor_head=False):\n",
    "    model = model\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            x = batch['features']\n",
    "            y = batch['labels']\n",
    "            t = batch['tutor'] if use_tutor_head else None\n",
    "\n",
    "            outputs = model(x)\n",
    "            task_outputs = {k: outputs[k] for k in ['mistake_identification', 'mistake_location', 'providing_guidance', 'actionability']}\n",
    "            tutor_output = outputs['tutor'] if use_tutor_head else None\n",
    "\n",
    "            loss = sum(loss_fns[dim](task_outputs[dim], y[dim]) for dim in task_outputs)\n",
    "\n",
    "            if use_tutor_head:\n",
    "                loss += loss_fn(tutor_output, t)\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        evaluate_model(model, val_loader, use_tutor_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 9.3328\n",
      "\n",
      " Classification Report for mistake_identification\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000        67\n",
      "           1      0.065     0.970     0.122        33\n",
      "           2      0.000     0.000     0.000       396\n",
      "\n",
      "    accuracy                          0.065       496\n",
      "   macro avg      0.022     0.323     0.041       496\n",
      "weighted avg      0.004     0.065     0.008       496\n",
      "\n",
      "\n",
      " Classification Report for mistake_location\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.014     0.028       142\n",
      "           1      0.071     1.000     0.132        35\n",
      "           2      0.000     0.000     0.000       319\n",
      "\n",
      "    accuracy                          0.075       496\n",
      "   macro avg      0.357     0.338     0.053       496\n",
      "weighted avg      0.291     0.075     0.017       496\n",
      "\n",
      "\n",
      " Classification Report for providing_guidance\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.750     0.025     0.049       119\n",
      "           1      0.000     0.000     0.000       104\n",
      "           2      0.555     1.000     0.714       273\n",
      "\n",
      "    accuracy                          0.556       496\n",
      "   macro avg      0.435     0.342     0.254       496\n",
      "weighted avg      0.485     0.556     0.405       496\n",
      "\n",
      "\n",
      " Classification Report for actionability\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       176\n",
      "           1      0.144     1.000     0.252        71\n",
      "           2      0.000     0.000     0.000       249\n",
      "\n",
      "    accuracy                          0.143       496\n",
      "   macro avg      0.048     0.333     0.084       496\n",
      "weighted avg      0.021     0.143     0.036       496\n",
      "\n",
      "Epoch 2/10 - Train Loss: 4.6693\n",
      "\n",
      " Classification Report for mistake_identification\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000        67\n",
      "           1      0.067     1.000     0.125        33\n",
      "           2      0.000     0.000     0.000       396\n",
      "\n",
      "    accuracy                          0.067       496\n",
      "   macro avg      0.022     0.333     0.042       496\n",
      "weighted avg      0.004     0.067     0.008       496\n",
      "\n",
      "\n",
      " Classification Report for mistake_location\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       142\n",
      "           1      0.071     1.000     0.132        35\n",
      "           2      0.000     0.000     0.000       319\n",
      "\n",
      "    accuracy                          0.071       496\n",
      "   macro avg      0.024     0.333     0.044       496\n",
      "weighted avg      0.005     0.071     0.009       496\n",
      "\n",
      "\n",
      " Classification Report for providing_guidance\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       119\n",
      "           1      0.000     0.000     0.000       104\n",
      "           2      0.550     1.000     0.710       273\n",
      "\n",
      "    accuracy                          0.550       496\n",
      "   macro avg      0.183     0.333     0.237       496\n",
      "weighted avg      0.303     0.550     0.391       496\n",
      "\n",
      "\n",
      " Classification Report for actionability\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       176\n",
      "           1      0.143     1.000     0.250        71\n",
      "           2      0.000     0.000     0.000       249\n",
      "\n",
      "    accuracy                          0.143       496\n",
      "   macro avg      0.048     0.333     0.083       496\n",
      "weighted avg      0.020     0.143     0.036       496\n",
      "\n",
      "Epoch 3/10 - Train Loss: 4.4761\n",
      "\n",
      " Classification Report for mistake_identification\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000        67\n",
      "           1      0.061     0.909     0.115        33\n",
      "           2      0.000     0.000     0.000       396\n",
      "\n",
      "    accuracy                          0.060       496\n",
      "   macro avg      0.020     0.303     0.038       496\n",
      "weighted avg      0.004     0.060     0.008       496\n",
      "\n",
      "\n",
      " Classification Report for mistake_location\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       142\n",
      "           1      0.071     1.000     0.132        35\n",
      "           2      0.000     0.000     0.000       319\n",
      "\n",
      "    accuracy                          0.071       496\n",
      "   macro avg      0.024     0.333     0.044       496\n",
      "weighted avg      0.005     0.071     0.009       496\n",
      "\n",
      "\n",
      " Classification Report for providing_guidance\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       119\n",
      "           1      0.000     0.000     0.000       104\n",
      "           2      0.550     0.993     0.708       273\n",
      "\n",
      "    accuracy                          0.546       496\n",
      "   macro avg      0.183     0.331     0.236       496\n",
      "weighted avg      0.303     0.546     0.389       496\n",
      "\n",
      "\n",
      " Classification Report for actionability\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       176\n",
      "           1      0.143     1.000     0.250        71\n",
      "           2      0.000     0.000     0.000       249\n",
      "\n",
      "    accuracy                          0.143       496\n",
      "   macro avg      0.048     0.333     0.083       496\n",
      "weighted avg      0.020     0.143     0.036       496\n",
      "\n",
      "Epoch 4/10 - Train Loss: 4.4701\n",
      "\n",
      " Classification Report for mistake_identification\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000        67\n",
      "           1      0.067     1.000     0.125        33\n",
      "           2      0.000     0.000     0.000       396\n",
      "\n",
      "    accuracy                          0.067       496\n",
      "   macro avg      0.022     0.333     0.042       496\n",
      "weighted avg      0.004     0.067     0.008       496\n",
      "\n",
      "\n",
      " Classification Report for mistake_location\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       142\n",
      "           1      0.071     1.000     0.132        35\n",
      "           2      0.000     0.000     0.000       319\n",
      "\n",
      "    accuracy                          0.071       496\n",
      "   macro avg      0.024     0.333     0.044       496\n",
      "weighted avg      0.005     0.071     0.009       496\n",
      "\n",
      "\n",
      " Classification Report for providing_guidance\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       119\n",
      "           1      0.000     0.000     0.000       104\n",
      "           2      0.550     1.000     0.710       273\n",
      "\n",
      "    accuracy                          0.550       496\n",
      "   macro avg      0.183     0.333     0.237       496\n",
      "weighted avg      0.303     0.550     0.391       496\n",
      "\n",
      "\n",
      " Classification Report for actionability\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       176\n",
      "           1      0.143     1.000     0.250        71\n",
      "           2      0.000     0.000     0.000       249\n",
      "\n",
      "    accuracy                          0.143       496\n",
      "   macro avg      0.048     0.333     0.083       496\n",
      "weighted avg      0.020     0.143     0.036       496\n",
      "\n",
      "Epoch 5/10 - Train Loss: 4.4241\n",
      "\n",
      " Classification Report for mistake_identification\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000        67\n",
      "           1      0.067     1.000     0.125        33\n",
      "           2      0.000     0.000     0.000       396\n",
      "\n",
      "    accuracy                          0.067       496\n",
      "   macro avg      0.022     0.333     0.042       496\n",
      "weighted avg      0.004     0.067     0.008       496\n",
      "\n",
      "\n",
      " Classification Report for mistake_location\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       142\n",
      "           1      0.071     1.000     0.132        35\n",
      "           2      0.000     0.000     0.000       319\n",
      "\n",
      "    accuracy                          0.071       496\n",
      "   macro avg      0.024     0.333     0.044       496\n",
      "weighted avg      0.005     0.071     0.009       496\n",
      "\n",
      "\n",
      " Classification Report for providing_guidance\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       119\n",
      "           1      0.000     0.000     0.000       104\n",
      "           2      0.550     1.000     0.710       273\n",
      "\n",
      "    accuracy                          0.550       496\n",
      "   macro avg      0.183     0.333     0.237       496\n",
      "weighted avg      0.303     0.550     0.391       496\n",
      "\n",
      "\n",
      " Classification Report for actionability\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       176\n",
      "           1      0.143     1.000     0.250        71\n",
      "           2      0.000     0.000     0.000       249\n",
      "\n",
      "    accuracy                          0.143       496\n",
      "   macro avg      0.048     0.333     0.083       496\n",
      "weighted avg      0.020     0.143     0.036       496\n",
      "\n",
      "Epoch 6/10 - Train Loss: 4.4344\n",
      "\n",
      " Classification Report for mistake_identification\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000        67\n",
      "           1      0.067     1.000     0.125        33\n",
      "           2      0.000     0.000     0.000       396\n",
      "\n",
      "    accuracy                          0.067       496\n",
      "   macro avg      0.022     0.333     0.042       496\n",
      "weighted avg      0.004     0.067     0.008       496\n",
      "\n",
      "\n",
      " Classification Report for mistake_location\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       142\n",
      "           1      0.071     1.000     0.132        35\n",
      "           2      0.000     0.000     0.000       319\n",
      "\n",
      "    accuracy                          0.071       496\n",
      "   macro avg      0.024     0.333     0.044       496\n",
      "weighted avg      0.005     0.071     0.009       496\n",
      "\n",
      "\n",
      " Classification Report for providing_guidance\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       119\n",
      "           1      0.000     0.000     0.000       104\n",
      "           2      0.550     1.000     0.710       273\n",
      "\n",
      "    accuracy                          0.550       496\n",
      "   macro avg      0.183     0.333     0.237       496\n",
      "weighted avg      0.303     0.550     0.391       496\n",
      "\n",
      "\n",
      " Classification Report for actionability\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       176\n",
      "           1      0.143     1.000     0.250        71\n",
      "           2      0.000     0.000     0.000       249\n",
      "\n",
      "    accuracy                          0.143       496\n",
      "   macro avg      0.048     0.333     0.083       496\n",
      "weighted avg      0.020     0.143     0.036       496\n",
      "\n",
      "Epoch 7/10 - Train Loss: 4.3919\n",
      "\n",
      " Classification Report for mistake_identification\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.200     0.015     0.028        67\n",
      "           1      0.163     0.424     0.235        33\n",
      "           2      0.830     0.848     0.839       396\n",
      "\n",
      "    accuracy                          0.708       496\n",
      "   macro avg      0.397     0.429     0.367       496\n",
      "weighted avg      0.700     0.708     0.689       496\n",
      "\n",
      "\n",
      " Classification Report for mistake_location\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       142\n",
      "           1      0.087     0.314     0.137        35\n",
      "           2      0.711     0.824     0.763       319\n",
      "\n",
      "    accuracy                          0.552       496\n",
      "   macro avg      0.266     0.380     0.300       496\n",
      "weighted avg      0.463     0.552     0.501       496\n",
      "\n",
      "\n",
      " Classification Report for providing_guidance\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       119\n",
      "           1      0.000     0.000     0.000       104\n",
      "           2      0.550     1.000     0.710       273\n",
      "\n",
      "    accuracy                          0.550       496\n",
      "   macro avg      0.183     0.333     0.237       496\n",
      "weighted avg      0.303     0.550     0.391       496\n",
      "\n",
      "\n",
      " Classification Report for actionability\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       176\n",
      "           1      0.151     0.366     0.214        71\n",
      "           2      0.531     0.691     0.600       249\n",
      "\n",
      "    accuracy                          0.399       496\n",
      "   macro avg      0.227     0.352     0.271       496\n",
      "weighted avg      0.288     0.399     0.332       496\n",
      "\n",
      "Epoch 8/10 - Train Loss: 4.3935\n",
      "\n",
      " Classification Report for mistake_identification\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.192     0.075     0.108        67\n",
      "           1      0.117     0.758     0.203        33\n",
      "           2      0.879     0.571     0.692       396\n",
      "\n",
      "    accuracy                          0.516       496\n",
      "   macro avg      0.396     0.468     0.334       496\n",
      "weighted avg      0.736     0.516     0.581       496\n",
      "\n",
      "\n",
      " Classification Report for mistake_location\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       142\n",
      "           1      0.082     0.857     0.149        35\n",
      "           2      0.705     0.285     0.406       319\n",
      "\n",
      "    accuracy                          0.244       496\n",
      "   macro avg      0.262     0.381     0.185       496\n",
      "weighted avg      0.459     0.244     0.272       496\n",
      "\n",
      "\n",
      " Classification Report for providing_guidance\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       119\n",
      "           1      0.000     0.000     0.000       104\n",
      "           2      0.550     1.000     0.710       273\n",
      "\n",
      "    accuracy                          0.550       496\n",
      "   macro avg      0.183     0.333     0.237       496\n",
      "weighted avg      0.303     0.550     0.391       496\n",
      "\n",
      "\n",
      " Classification Report for actionability\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       176\n",
      "           1      0.143     1.000     0.250        71\n",
      "           2      0.000     0.000     0.000       249\n",
      "\n",
      "    accuracy                          0.143       496\n",
      "   macro avg      0.048     0.333     0.083       496\n",
      "weighted avg      0.020     0.143     0.036       496\n",
      "\n",
      "Epoch 9/10 - Train Loss: 4.3907\n",
      "\n",
      " Classification Report for mistake_identification\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.176     0.716     0.282        67\n",
      "           1      0.000     0.000     0.000        33\n",
      "           2      0.888     0.500     0.640       396\n",
      "\n",
      "    accuracy                          0.496       496\n",
      "   macro avg      0.355     0.405     0.307       496\n",
      "weighted avg      0.733     0.496     0.549       496\n",
      "\n",
      "\n",
      " Classification Report for mistake_location\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       142\n",
      "           1      0.080     0.686     0.143        35\n",
      "           2      0.759     0.464     0.576       319\n",
      "\n",
      "    accuracy                          0.347       496\n",
      "   macro avg      0.280     0.383     0.240       496\n",
      "weighted avg      0.494     0.347     0.380       496\n",
      "\n",
      "\n",
      " Classification Report for providing_guidance\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       119\n",
      "           1      0.000     0.000     0.000       104\n",
      "           2      0.550     1.000     0.710       273\n",
      "\n",
      "    accuracy                          0.550       496\n",
      "   macro avg      0.183     0.333     0.237       496\n",
      "weighted avg      0.303     0.550     0.391       496\n",
      "\n",
      "\n",
      " Classification Report for actionability\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       176\n",
      "           1      0.000     0.000     0.000        71\n",
      "           2      0.502     1.000     0.668       249\n",
      "\n",
      "    accuracy                          0.502       496\n",
      "   macro avg      0.167     0.333     0.223       496\n",
      "weighted avg      0.252     0.502     0.336       496\n",
      "\n",
      "Epoch 10/10 - Train Loss: 4.3892\n",
      "\n",
      " Classification Report for mistake_identification\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.149     0.851     0.253        67\n",
      "           1      0.000     0.000     0.000        33\n",
      "           2      0.885     0.253     0.393       396\n",
      "\n",
      "    accuracy                          0.317       496\n",
      "   macro avg      0.345     0.368     0.215       496\n",
      "weighted avg      0.727     0.317     0.348       496\n",
      "\n",
      "\n",
      " Classification Report for mistake_location\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       142\n",
      "           1      0.074     0.829     0.136        35\n",
      "           2      0.777     0.251     0.379       319\n",
      "\n",
      "    accuracy                          0.220       496\n",
      "   macro avg      0.283     0.360     0.172       496\n",
      "weighted avg      0.505     0.220     0.253       496\n",
      "\n",
      "\n",
      " Classification Report for providing_guidance\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       119\n",
      "           1      0.000     0.000     0.000       104\n",
      "           2      0.550     1.000     0.710       273\n",
      "\n",
      "    accuracy                          0.550       496\n",
      "   macro avg      0.183     0.333     0.237       496\n",
      "weighted avg      0.303     0.550     0.391       496\n",
      "\n",
      "\n",
      " Classification Report for actionability\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       176\n",
      "           1      0.000     0.000     0.000        71\n",
      "           2      0.502     1.000     0.668       249\n",
      "\n",
      "    accuracy                          0.502       496\n",
      "   macro avg      0.167     0.333     0.223       496\n",
      "weighted avg      0.252     0.502     0.336       496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = TutorClassifier(len(feature_cols))\n",
    "train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, use_tutor_head=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it s worse when i add bert embeddings :(\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
