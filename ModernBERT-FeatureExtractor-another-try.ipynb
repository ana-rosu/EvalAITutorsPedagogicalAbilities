{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ModernBert feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT embeddings (mean): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:18<00:00,  8.23it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'mistake_identification_label'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ana\\miniconda3\\envs\\deep-l\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'mistake_identification_label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     58\u001b[39m X_bert_mean = np.array(df[\u001b[33m'\u001b[39m\u001b[33mresponse_embeddings_mean\u001b[39m\u001b[33m'\u001b[39m].tolist())\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# X_bert_max = np.array(df['response_embeddings_max'].tolist())\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# X_bert_cls_mean = np.array(df['response_embeddings_cls+mean'].tolist())\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m y_task1 = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_columns\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.values\n\u001b[32m     63\u001b[39m y_task2 = df[target_columns[\u001b[32m1\u001b[39m]].values\n\u001b[32m     64\u001b[39m y_task3 = df[target_columns[\u001b[32m2\u001b[39m]].values\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ana\\miniconda3\\envs\\deep-l\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ana\\miniconda3\\envs\\deep-l\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'mistake_identification_label'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv('data/df_eda.csv')\n",
    "label_map = {\"No\": 0, \"To some extent\": 1, \"Yes\": 2}\n",
    "for metric in [\"mistake_identification\", \"mistake_location\", \"providing_guidance\", \"actionability\"]:\n",
    "    df[metric + \"_label\"] = df[metric].map(label_map)\n",
    "target_columns = ['mistake_identification_label', 'mistake_location_label', 'providing_guidance_label', 'actionability_label']\n",
    "\n",
    "model_id = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "modernBert = AutoModel.from_pretrained(model_id)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def extract_embeddings(texts, model, tokenizer, method=\"cls\", batch_size=16, max_length=512):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=f\"Extracting BERT embeddings ({method})\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            hidden_states = outputs.last_hidden_state  # shape: (batch, seq_len, hidden)\n",
    "\n",
    "        if method == \"cls\":\n",
    "            batch_embeddings = hidden_states[:, 0, :]  # [CLS] token\n",
    "        elif method == \"mean\":\n",
    "            batch_embeddings = hidden_states.mean(dim=1)\n",
    "        elif method == \"max\":\n",
    "            batch_embeddings = hidden_states.max(dim=1).values\n",
    "        elif method == \"cls+mean\":\n",
    "            cls = hidden_states[:, 0, :]\n",
    "            mean = hidden_states.mean(dim=1)\n",
    "            batch_embeddings = torch.cat([cls, mean], dim=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method '{method}'. Choose from: 'cls', 'mean', 'max', 'cls+mean'.\")\n",
    "\n",
    "        embeddings.extend(batch_embeddings.cpu().numpy())\n",
    "\n",
    "    return np.array(embeddings)\n",
    "\n",
    "\n",
    "# df['response_embeddings_cls'] = list(extract_embeddings(df['response'].tolist(), model=modernBert, tokenizer=tokenizer, method=\"cls\"))\n",
    "df['response_embeddings_mean'] = list(extract_embeddings(df['response'].tolist(), model=modernBert, tokenizer=tokenizer, method=\"mean\"))\n",
    "# df['response_embeddings_max'] = list(extract_embeddings(df['response'].tolist(), model=modernBert, tokenizer=tokenizer, method=\"max\"))\n",
    "# df['response_embeddings_cls+mean'] = list(extract_embeddings(df['response'].tolist(), model=modernBert, tokenizer=tokenizer, method=\"cls+mean\"))\n",
    "\n",
    "# X_bert_cls = np.array(df['response_embeddings_cls'].tolist())\n",
    "X_bert_mean = np.array(df['response_embeddings_mean'].tolist())\n",
    "# X_bert_max = np.array(df['response_embeddings_max'].tolist())\n",
    "# X_bert_cls_mean = np.array(df['response_embeddings_cls+mean'].tolist())\n",
    "\n",
    "y_task1 = df[target_columns[0]].values\n",
    "y_task2 = df[target_columns[1]].values\n",
    "y_task3 = df[target_columns[2]].values\n",
    "y_task4 = df[target_columns[3]].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# np.save('data/embeddings/X_bert_cls.npy', X_bert_cls)\n",
    "np.save('data/embeddings/X_bert_mean.npy', X_bert_mean)\n",
    "# np.save('data/embeddings/X_bert_max.npy', X_bert_max)\n",
    "# np.save('data/embeddings/X_bert_cls_mean.npy', X_bert_cls_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT embeddings (attention pooling): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:16<00:00,  9.32it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.attention_vector = nn.Parameter(torch.randn(hidden_size))\n",
    "\n",
    "    def forward(self, hidden_states, mask):\n",
    "        # hidden_states: (batch, seq_len, hidden_size)\n",
    "        # mask: (batch, seq_len) â€” 1 for real tokens, 0 for padding\n",
    "        self.attention_vector = self.attention_vector.to(hidden_states.device)\n",
    "        # Compute token-level attention scores: (batch, seq_len)\n",
    "        scores = torch.matmul(hidden_states, self.attention_vector)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)  # Mask out padding tokens\n",
    "        weights = F.softmax(scores, dim=1)\n",
    "\n",
    "        # Apply weights to hidden states: (batch, hidden_size)\n",
    "        weighted_output = torch.sum(hidden_states * weights.unsqueeze(-1), dim=1)\n",
    "        return weighted_output\n",
    "\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "attention_pooling = AttentionPooling(hidden_size=model.config.hidden_size).to(device)\n",
    "\n",
    "def extract_embeddings_attention(texts, model, tokenizer, batch_size=16, max_length=512):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=f\"Extracting BERT embeddings (attention pooling)\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            hidden_states = outputs.last_hidden_state  # shape: (batch, seq_len, hidden)\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            batch_embeddings = attention_pooling(hidden_states, attention_mask)\n",
    "\n",
    "        embeddings.extend(batch_embeddings.cpu().numpy())\n",
    "\n",
    "    return np.array(embeddings)\n",
    "\n",
    "df['response_embeddings_attention'] = list(extract_embeddings_attention(df['response'].tolist(), model=model, tokenizer=tokenizer))\n",
    "X_bert_attention = np.array(df['response_embeddings_attention'].tolist())\n",
    "np.save('data/embeddings/X_bert_attention.npy', X_bert_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/df_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== BERT [CLS] =====\n",
      "Task 1 (Mistake Identification) - Avg Macro F1: 0.5818\n",
      "Task 2 (Mistake Location) - Avg Macro F1: 0.4696\n",
      "Task 3 (Providing Guidance) - Avg Macro F1: 0.4928\n",
      "Task 4 (Actionability) - Avg Macro F1: 0.5420\n",
      "\n",
      "===== BERT Mean =====\n",
      "Task 1 (Mistake Identification) - Avg Macro F1: 0.6000\n",
      "Task 2 (Mistake Location) - Avg Macro F1: 0.5042\n",
      "Task 3 (Providing Guidance) - Avg Macro F1: 0.4999\n",
      "Task 4 (Actionability) - Avg Macro F1: 0.5844\n",
      "\n",
      "===== BERT Max =====\n",
      "Task 1 (Mistake Identification) - Avg Macro F1: 0.5540\n",
      "Task 2 (Mistake Location) - Avg Macro F1: 0.4603\n",
      "Task 3 (Providing Guidance) - Avg Macro F1: 0.4568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ana\\miniconda3\\envs\\deep-l\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 4 (Actionability) - Avg Macro F1: 0.5240\n",
      "\n",
      "===== BERT [CLS]+Mean =====\n",
      "Task 1 (Mistake Identification) - Avg Macro F1: 0.6067\n",
      "Task 2 (Mistake Location) - Avg Macro F1: 0.5068\n",
      "Task 3 (Providing Guidance) - Avg Macro F1: 0.5046\n",
      "Task 4 (Actionability) - Avg Macro F1: 0.5823\n",
      "\n",
      "===== BERT Attention-Pooled =====\n",
      "Task 1 (Mistake Identification) - Avg Macro F1: 0.5233\n",
      "Task 2 (Mistake Location) - Avg Macro F1: 0.4380\n",
      "Task 3 (Providing Guidance) - Avg Macro F1: 0.4500\n",
      "Task 4 (Actionability) - Avg Macro F1: 0.4872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BERT [CLS]': {'Task 1 (Mistake Identification)': [0.7870833333333334,\n",
       "   0.5818010135553957],\n",
       "  'Task 2 (Mistake Location)': [0.5870833333333334, 0.4695766949091082],\n",
       "  'Task 3 (Providing Guidance)': [0.5345833333333334, 0.4928147675305268],\n",
       "  'Task 4 (Actionability)': [0.5975, 0.5419969882383128]},\n",
       " 'BERT Mean': {'Task 1 (Mistake Identification)': [0.8012500000000001,\n",
       "   0.600008934220882],\n",
       "  'Task 2 (Mistake Location)': [0.6224999999999999, 0.5042206193009685],\n",
       "  'Task 3 (Providing Guidance)': [0.5483333333333333, 0.49993909881447396],\n",
       "  'Task 4 (Actionability)': [0.64125, 0.5843853493663623]},\n",
       " 'BERT Max': {'Task 1 (Mistake Identification)': [0.79125, 0.5540090329313057],\n",
       "  'Task 2 (Mistake Location)': [0.5879166666666668, 0.46031579693058544],\n",
       "  'Task 3 (Providing Guidance)': [0.5104166666666666, 0.456823232036338],\n",
       "  'Task 4 (Actionability)': [0.5912499999999999, 0.5240188662769023]},\n",
       " 'BERT [CLS]+Mean': {'Task 1 (Mistake Identification)': [0.8233333333333335,\n",
       "   0.6067062211791223],\n",
       "  'Task 2 (Mistake Location)': [0.6383333333333333, 0.5068159787749723],\n",
       "  'Task 3 (Providing Guidance)': [0.5608333333333333, 0.5045529735934711],\n",
       "  'Task 4 (Actionability)': [0.655, 0.582315746444092]},\n",
       " 'BERT Attention-Pooled': {'Task 1 (Mistake Identification)': [0.6983333333333334,\n",
       "   0.5233048527931509],\n",
       "  'Task 2 (Mistake Location)': [0.5275, 0.4380385611459702],\n",
       "  'Task 3 (Providing Guidance)': [0.5008333333333332, 0.45004748300608066],\n",
       "  'Task 4 (Actionability)': [0.5354166666666667, 0.48717054313641184]}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_embeddings(X, y, name):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    f1_scores = []\n",
    "    acc_scores = []\n",
    "\n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        clf = LogisticRegression(max_iter=2000, class_weight='balanced', random_state=42)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        acc_scores.append(acc)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    avg_acc = np.mean(acc_scores)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    print(f\"{name} - Avg Macro F1: {avg_f1:.4f}\")\n",
    "    return avg_acc, avg_f1\n",
    "\n",
    "# Load embeddings from dataframe\n",
    "X_bert_cls = np.array(df['response_embeddings_cls'].tolist())\n",
    "X_bert_mean = np.array(df['response_embeddings_mean'].tolist())\n",
    "X_bert_max = np.array(df['response_embeddings_max'].tolist())\n",
    "X_bert_cls_mean = np.array(df['response_embeddings_cls+mean'].tolist())\n",
    "X_bert_attention = np.array(df['response_embeddings_attention'].tolist())\n",
    "\n",
    "# Load targets\n",
    "targets = {\n",
    "    \"Task 1 (Mistake Identification)\": df[target_columns[0]].values,\n",
    "    \"Task 2 (Mistake Location)\": df[target_columns[1]].values,\n",
    "    \"Task 3 (Providing Guidance)\": df[target_columns[2]].values,\n",
    "    \"Task 4 (Actionability)\": df[target_columns[3]].values\n",
    "}\n",
    "\n",
    "# Embeddings dictionary\n",
    "embeddings = {\n",
    "    \"BERT [CLS]\": X_bert_cls,\n",
    "    \"BERT Mean\": X_bert_mean,\n",
    "    \"BERT Max\": X_bert_max,\n",
    "    \"BERT [CLS]+Mean\": X_bert_cls_mean,\n",
    "    \"BERT Attention-Pooled\": X_bert_attention\n",
    "}\n",
    "\n",
    "# Evaluate each embedding type across all tasks\n",
    "results = {}\n",
    "for emb_name, X_emb in embeddings.items():\n",
    "    print(f\"\\n===== {emb_name} =====\")\n",
    "    results[emb_name] = {}\n",
    "    for task_name, y in targets.items():\n",
    "        acc, f1 = evaluate_embeddings(X_emb, y, task_name)\n",
    "        results[emb_name][task_name] = [acc,f1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from above => mean+cls best for all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below => mean is the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== BERT [CLS] =====\n",
      "Task 1 (Mistake Identification) - Avg Accuracy: 0.7096, Avg Macro F1: 0.5253\n",
      "Task 2 (Mistake Location) - Avg Accuracy: 0.5462, Avg Macro F1: 0.4502\n",
      "Task 3 (Providing Guidance) - Avg Accuracy: 0.4950, Avg Macro F1: 0.4579\n",
      "Task 4 (Actionability) - Avg Accuracy: 0.5646, Avg Macro F1: 0.5115\n",
      "\n",
      "===== BERT Mean =====\n",
      "Task 1 (Mistake Identification) - Avg Accuracy: 0.7329, Avg Macro F1: 0.5527\n",
      "Task 2 (Mistake Location) - Avg Accuracy: 0.5733, Avg Macro F1: 0.4769\n",
      "Task 3 (Providing Guidance) - Avg Accuracy: 0.5075, Avg Macro F1: 0.4666\n",
      "Task 4 (Actionability) - Avg Accuracy: 0.5867, Avg Macro F1: 0.5360\n",
      "\n",
      "===== BERT Max =====\n",
      "Task 1 (Mistake Identification) - Avg Accuracy: 0.7100, Avg Macro F1: 0.5182\n",
      "Task 2 (Mistake Location) - Avg Accuracy: 0.5388, Avg Macro F1: 0.4465\n",
      "Task 3 (Providing Guidance) - Avg Accuracy: 0.5037, Avg Macro F1: 0.4649\n",
      "Task 4 (Actionability) - Avg Accuracy: 0.5675, Avg Macro F1: 0.5198\n",
      "\n",
      "===== BERT [CLS]+Mean =====\n",
      "Task 1 (Mistake Identification) - Avg Accuracy: 0.6467, Avg Macro F1: 0.4746\n",
      "Task 2 (Mistake Location) - Avg Accuracy: 0.5104, Avg Macro F1: 0.4196\n",
      "Task 3 (Providing Guidance) - Avg Accuracy: 0.4433, Avg Macro F1: 0.4082\n",
      "Task 4 (Actionability) - Avg Accuracy: 0.4787, Avg Macro F1: 0.4289\n",
      "\n",
      "===== BERT Attention-Pooled =====\n",
      "Task 1 (Mistake Identification) - Avg Accuracy: 0.5933, Avg Macro F1: 0.4475\n",
      "Task 2 (Mistake Location) - Avg Accuracy: 0.5179, Avg Macro F1: 0.4220\n",
      "Task 3 (Providing Guidance) - Avg Accuracy: 0.4762, Avg Macro F1: 0.4152\n",
      "Task 4 (Actionability) - Avg Accuracy: 0.5146, Avg Macro F1: 0.4512\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def evaluate_embeddings_with_lda_cv(X, y, name, n_components=2):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    f1_scores, acc_scores = [], []\n",
    "\n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Build a pipeline: scale -> LDA -> Logistic Regression\n",
    "        pipeline = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"lda\", LinearDiscriminantAnalysis(n_components=n_components)),\n",
    "            (\"clf\", LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42))\n",
    "        ])\n",
    "\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1_scores.append(f1)\n",
    "        acc_scores.append(acc)\n",
    "\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    avg_acc = np.mean(acc_scores)\n",
    "    print(f\"{name} - Avg Accuracy: {avg_acc:.4f}, Avg Macro F1: {avg_f1:.4f}\")\n",
    "    return avg_acc, avg_f1\n",
    "\n",
    "\n",
    "# Evaluate each embedding type across all tasks\n",
    "results = {}\n",
    "for emb_name, X_emb in embeddings.items():\n",
    "    print(f\"\\n===== {emb_name} =====\")\n",
    "    results[emb_name] = {}\n",
    "    for task_name, y in targets.items():\n",
    "        acc, f1 = evaluate_embeddings_with_lda_cv(X_emb, y, task_name)\n",
    "        results[emb_name][task_name] = [acc,f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== BERT [CLS] (MLP) =====\n",
      "Task 1 (Mistake Identification) - Avg Macro F1: 0.5518, Avg Accuracy: 0.8650\n",
      "Task 2 (Mistake Location) - Avg Macro F1: 0.4283, Avg Accuracy: 0.7079\n",
      "Task 3 (Providing Guidance) - Avg Macro F1: 0.4411, Avg Accuracy: 0.6375\n",
      "Task 4 (Actionability) - Avg Macro F1: 0.4636, Avg Accuracy: 0.6521\n",
      "\n",
      "===== BERT Mean (MLP) =====\n",
      "Task 1 (Mistake Identification) - Avg Macro F1: 0.5498, Avg Accuracy: 0.8642\n",
      "Task 2 (Mistake Location) - Avg Macro F1: 0.4668, Avg Accuracy: 0.7029\n",
      "Task 3 (Providing Guidance) - Avg Macro F1: 0.4423, Avg Accuracy: 0.6338\n",
      "Task 4 (Actionability) - Avg Macro F1: 0.5387, Avg Accuracy: 0.6929\n",
      "\n",
      "===== BERT Max (MLP) =====\n",
      "Task 1 (Mistake Identification) - Avg Macro F1: 0.5487, Avg Accuracy: 0.8692\n",
      "Task 2 (Mistake Location) - Avg Macro F1: 0.4296, Avg Accuracy: 0.7100\n",
      "Task 3 (Providing Guidance) - Avg Macro F1: 0.4296, Avg Accuracy: 0.6396\n",
      "Task 4 (Actionability) - Avg Macro F1: 0.4728, Avg Accuracy: 0.6604\n",
      "\n",
      "===== BERT [CLS]+Mean (MLP) =====\n",
      "Task 1 (Mistake Identification) - Avg Macro F1: 0.5702, Avg Accuracy: 0.8671\n",
      "Task 2 (Mistake Location) - Avg Macro F1: 0.4490, Avg Accuracy: 0.7075\n",
      "Task 3 (Providing Guidance) - Avg Macro F1: 0.4528, Avg Accuracy: 0.6462\n",
      "Task 4 (Actionability) - Avg Macro F1: 0.5712, Avg Accuracy: 0.7092\n",
      "\n",
      "===== BERT Attention-Pooled (MLP) =====\n",
      "Task 1 (Mistake Identification) - Avg Macro F1: 0.4606, Avg Accuracy: 0.8242\n",
      "Task 2 (Mistake Location) - Avg Macro F1: 0.3671, Avg Accuracy: 0.6642\n",
      "Task 3 (Providing Guidance) - Avg Macro F1: 0.3718, Avg Accuracy: 0.5900\n",
      "Task 4 (Actionability) - Avg Macro F1: 0.4285, Avg Accuracy: 0.6025\n"
     ]
    }
   ],
   "source": [
    "def evaluate_embeddings_mlp(X, y, name):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    f1_scores = []\n",
    "    acc_scores = []\n",
    "\n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        clf = MLPClassifier(\n",
    "            hidden_layer_sizes=(256, 128),\n",
    "            max_iter=500,\n",
    "            early_stopping=True,\n",
    "            learning_rate='adaptive',\n",
    "            random_state=42\n",
    "        )\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        acc_scores.append(acc)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    avg_acc = np.mean(acc_scores)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    print(f\"{name} - Avg Macro F1: {avg_f1:.4f}, Avg Accuracy: {avg_acc:.4f}\")\n",
    "    return avg_acc, avg_f1\n",
    "\n",
    "# Evaluate all embeddings and tasks\n",
    "results_mlp = {}\n",
    "for emb_name, X_emb in embeddings.items():\n",
    "    print(f\"\\n===== {emb_name} (MLP) =====\")\n",
    "    results_mlp[emb_name] = {}\n",
    "    for task_name, y in targets.items():\n",
    "        acc, f1 = evaluate_embeddings_mlp(X_emb, y, task_name)\n",
    "        results_mlp[emb_name][task_name] = [acc, f1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "improved accuracy compared to log reg\n",
    "cls+mean seems to be the best\n",
    "\n",
    "current best for actionability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try adding last student utterance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ModernBERT model\n",
    "model_id = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Create concatenated input: (LSU + [SEP] + Response)\n",
    "df[\"input_text\"] = df[\"last_student_utterance\"] + \" [SEP] \" + df[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting CLS Embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [01:21<00:00,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš ï¸ 144 out of 2400 texts were truncated.\n",
      "ðŸ”Ž Sample truncated examples:\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Â 75 [SEP] That's almost right, but remember to multiply the numbers step by step, so 5 times 4 equals 20, and then add the 5 times 10 which is 50, giving us a total of 70. Let's try that together next time!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Â Yes 30*10 is 1300. [SEP] Tutor response: That's correct, 30 multiplied by 10 equals 300, not 1300.\n",
      "\n",
      "\n",
      "### Assistant:\n",
      "Tutor response (maximum one sentence):\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Â is that a 20 [SEP] That's okay! Remember, there are actually 1,000 milliliters in one literâ€”so think of it like adding three zeros to 1 for the conversion.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Â We can work this out by taking the cost of the wife's ring (which is twice the cost of Jim's ring) and subtracting the amount he made from selling his ring (which is half the cost of his ring). This gives us $20,000 - $5,000 = $15,000. [SEP] To solve this problem, let's first understand the concept of subtraction as finding the difference between two numbers.\n",
      "\n",
      "### Student:\n",
      "I'm having trouble understanding how to find the difference between two numbers using subtraction. Can you explain it to me...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Â =29 [SEP] Great job! You've earned 3 points and your total is now 103.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_cls_embeddings(texts, batch_size=16):\n",
    "    embeddings = []\n",
    "    truncated_examples = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting CLS Embeddings\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "\n",
    "        encoded = tokenizer(\n",
    "            batch,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_length=True\n",
    "        )\n",
    "        lengths = encoded.pop(\"length\") \n",
    "        encoded.pop(\"overflow_to_sample_mapping\", None)\n",
    "      \n",
    "        for j, length in enumerate(lengths[:len(batch)]):  # ensure match\n",
    "            if length >= 512:\n",
    "                truncated_examples.append(batch[j])\n",
    "\n",
    "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "            cls = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.extend(cls)\n",
    "\n",
    "    # Print truncated examples summary\n",
    "    print(f\"\\n {len(truncated_examples)} out of {len(texts)} texts were truncated.\")\n",
    "    print(\"Sample truncated examples:\\n\")\n",
    "    for example in truncated_examples[:5]:  # print first 5\n",
    "        print(\"-\" * 80)\n",
    "        print(example[:500] + (\"...\" if len(example) > 500 else \"\"))  # print first 500 chars\n",
    "        print()\n",
    "\n",
    "    return np.array(embeddings)\n",
    "\n",
    "\n",
    "# Extract CLS embeddings\n",
    "X_lsu_bert_cls = extract_cls_embeddings(df[\"input_text\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Results:\n",
      "Task 1 (Mistake Identification): Accuracy = 0.8071, Macro F1 = 0.3804\n",
      "Task 2 (Mistake Location): Accuracy = 0.6750, Macro F1 = 0.4141\n",
      "Task 3 (Providing Guidance): Accuracy = 0.5979, Macro F1 = 0.4003\n",
      "Task 4 (Actionability): Accuracy = 0.6250, Macro F1 = 0.4502\n"
     ]
    }
   ],
   "source": [
    "def evaluate_embeddings_mlp(X, y, name):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    f1_scores = []\n",
    "    acc_scores = []\n",
    "\n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        clf = MLPClassifier(\n",
    "            hidden_layer_sizes=(256, 128),\n",
    "            max_iter=500,\n",
    "            early_stopping=True,\n",
    "            learning_rate='adaptive',\n",
    "            random_state=42\n",
    "        )\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        acc_scores.append(acc)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return np.mean(acc_scores), np.mean(f1_scores)\n",
    "\n",
    "results = {}\n",
    "for task_name, y in targets.items():\n",
    "    acc, f1 = evaluate_embeddings_mlp(X_lsu_bert_cls, y, task_name)\n",
    "    results[task_name] = (acc, f1)\n",
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "for task, (acc, f1) in results.items():\n",
    "    print(f\"{task}: Accuracy = {acc:.4f}, Macro F1 = {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Task 1 (Mistake Identification) -  Avg Accuracy: 0.8650, Avg Macro F1: 0.5518,\n",
    "Task 2 (Mistake Location) -             Avg Accuracy: 0.7079, Avg Macro F1: 0.4283\n",
    "Task 3 (Providing Guidance) -           Avg Accuracy: 0.6375, Avg Macro F1: 0.4411, \n",
    "Task 4 (Actionability) -                Avg Accuracy: 0.6521, Avg Macro F1: 0.4636,  -->\n",
    "using only responses embedding and cls method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/embeddings/X_lsu_bert_cls.npy', X_lsu_bert_cls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
